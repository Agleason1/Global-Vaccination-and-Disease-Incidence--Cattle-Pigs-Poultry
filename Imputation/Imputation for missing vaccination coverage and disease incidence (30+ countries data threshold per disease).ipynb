{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0763ba87",
   "metadata": {},
   "source": [
    "## Kernel to load: vax_inc_general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f68f29e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import pycountry\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.utils import resample\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import optuna \n",
    "import logging\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e62fd038",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_dir = os.path.dirname(os.getcwd())\n",
    "source_data_path=os.path.join(notebook_dir, \"Common Source Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "039a2ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_ghd_region=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feb55080",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = {\n",
    "    \"Andean Latin America\": [\n",
    "        \"Bolivia\", \"Ecuador\", \"Peru\"\n",
    "    ],\n",
    "    \"Australasia\": [\n",
    "        \"Australia\", \"New Zealand\", \"American Samoa\", \"Cook Islands\", \"Tokelau\"\n",
    "    ],\n",
    "    \"Caribbean\": [\n",
    "        \"Antigua and Barbuda\", \"Bahamas\", \"Barbados\", \"Belize\", \"Cuba\", \"Dominica\",\n",
    "        \"Dominican Republic\", \"Grenada\", \"Guyana\", \"Haiti\", \"Jamaica\", \n",
    "        \"Saint Kitts and Nevis\", \"Saint Lucia\", \"Saint Vincent and the Grenadines\", \n",
    "        \"Suriname\", \"Trinidad and Tobago\", \"Aruba\", \"Curaçao\", \"Saint Martin (French part)\", \n",
    "        \"Sint Maarten (Dutch part)\", \"Turks and Caicos Islands\", \"Virgin Islands, British\", \n",
    "        \"Virgin Islands, U.S.\",'Martinique','Guadeloupe'\n",
    "    ],\n",
    "    \"Central Asia\": [\n",
    "        \"Armenia\", \"Azerbaijan\", \"Georgia\", \"Kazakhstan\", \"Kyrgyzstan\", \"Mongolia\", \n",
    "        \"Tajikistan\", \"Turkmenistan\", \"Uzbekistan\"\n",
    "    ],\n",
    "    \"Central Europe\": [\n",
    "        \"Albania\", \"Bosnia and Herzegovina\", \"Bulgaria\", \"Croatia\", \"Czechia\", \n",
    "        \"Hungary\", \"Kosovo\", \"Montenegro\", \"North Macedonia\", \"Poland\", \"Romania\", \n",
    "        \"Serbia\", \"Serbia and Montenegro\", \"Slovakia\", \"Slovenia\"\n",
    "    ],\n",
    "    \"Central Latin America\": [\n",
    "        \"Colombia\", \"Costa Rica\", \"El Salvador\", \"Guatemala\", \"Honduras\", \"Mexico\", \n",
    "        \"Nicaragua\", \"Panama\", \"Venezuela\"\n",
    "    ],\n",
    "    \"Central Sub-Saharan Africa\": [\n",
    "        \"Angola\", \"Central African Republic\", \"Congo\", \"Democratic Republic of the Congo\", \n",
    "        \"Equatorial Guinea\", \"Gabon\"\n",
    "    ],\n",
    "    \"East Asia\": [\n",
    "        \"China\", \"Democratic People's Republic of Korea\", \"Taiwan\", \"Hong Kong\", \"Macao\"\n",
    "    ],\n",
    "    \"Eastern Europe\": [\n",
    "        \"Belarus\", \"Estonia\", \"Latvia\", \"Lithuania\", \"Republic of Moldova\", \n",
    "        \"Russian Federation\", \"Ukraine\"\n",
    "    ],\n",
    "    \"Eastern Sub-Saharan Africa\": [\n",
    "        \"Burundi\", \"Comoros\", \"Djibouti\", \"Eritrea\", \"Ethiopia\", \"Kenya\", \"Madagascar\", \n",
    "        \"Malawi\", \"Mozambique\", \"Rwanda\", \"Somalia\", \"South Sudan\", \"Uganda\", \n",
    "        \"United Republic of Tanzania\", \"Zambia\", \"Réunion\"\n",
    "    ],\n",
    "    \"High-income Asia Pacific\": [\n",
    "        \"Brunei Darussalam\", \"Japan\", \"Republic of Korea\", \"Singapore\", \"Guam\", \n",
    "        \"Northern Mariana Islands\"\n",
    "    ],\n",
    "    \"High-income North America\": [\n",
    "        \"Canada\", \"United States of America\", \"Bermuda\", \"Puerto Rico\", \"Greenland\", \n",
    "        \"Cayman Islands\"\n",
    "    ],\n",
    "    \"North Africa and Middle East\": [\n",
    "        \"Afghanistan\", \"Algeria\", \"Bahrain\", \"Egypt\", \"Iran\", \"Iraq\", \"Jordan\", \n",
    "        \"Kuwait\", \"Lebanon\", \"Libya\", \"Morocco\", \"Oman\", \"Palestine\", \"Qatar\", \n",
    "        \"Saudi Arabia\", \"Sudan\", \"Syrian Arab Republic\", \"Tunisia\", \"Türkiye\", \n",
    "        \"United Arab Emirates\", \"Yemen\", \"Israel\"\n",
    "    ],\n",
    "    \"Oceania\": [\n",
    "        \"Fiji\", \"Kiribati\", \"Marshall Islands\", \"Micronesia\", \"Nauru\", \"Niue\", \n",
    "        \"Palau\", \"Papua New Guinea\", \"Samoa\", \"Solomon Islands\", \"Tonga\", \n",
    "        \"Tuvalu\", \"Vanuatu\", \"New Caledonia\", \"French Polynesia\"\n",
    "    ],\n",
    "    \"Southern Latin America\": [\n",
    "            \"Argentina\", \"Chile\", \"Uruguay\"\n",
    "    ],\n",
    "    \"Southern Sub-Saharan Africa\": [\n",
    "        \"Botswana\", \"Eswatini\", \"Lesotho\", \"Namibia\", \"South Africa\", \"Zimbabwe\"\n",
    "    ],\n",
    "    \"Tropical Latin America\": [\n",
    "        \"Brazil\", \"Paraguay\",'French Guiana'\n",
    "    ],\n",
    "    \"Western Europe\": [\n",
    "        \"Andorra\", \"Austria\", \"Belgium\", \"Cyprus\", \"Denmark\", \"Finland\", \"France\", \n",
    "        \"Germany\", \"Greece\", \"Guernsey\", \"Iceland\", \"Ireland\", \"Isle of Man\",\n",
    "        \"Italy\", \"Jersey\", \"Liechtenstein\", \"Luxembourg\", \"Malta\", \"Monaco\", \n",
    "        \"Netherlands\", \"Norway\", \"Portugal\", \"San Marino\", \"Spain\", \"Sweden\", \n",
    "        \"Switzerland\", \"United Kingdom\", \"Gibraltar\", \"Faroe Islands\"\n",
    "    ],\n",
    "    \"Southeast Asia\":[\n",
    "    'Cambodia','Indonesia',\"Lao People's Democratic Republic\",'Malaysia','Maldives',\n",
    "    'Mauritius','Myanmar','Philippines','Seychelles','Sri Lanka','Thailand','Timor-Leste',\n",
    "    'Viet Nam'\n",
    "    ],\n",
    "    \"South Asia\":[\n",
    "    'Bangladesh','Bhutan','India','Nepal','Pakistan'\n",
    "    ],\n",
    "    \"Western Sub-Saharan Africa\": [\n",
    "        \"Benin\", \"Burkina Faso\", \"Cabo Verde\", \"Cameroon\", \"Chad\", \"Côte d'Ivoire\", \n",
    "        \"Gambia\", \"Ghana\", \"Guinea\", \"Guinea-Bissau\", \"Liberia\", \"Mali\", \"Mauritania\", \n",
    "        \"Niger\", \"Nigeria\", \"Sao Tome and Principe\", \"Senegal\", \"Sierra Leone\", \"Togo\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21ecc337",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for region, countries in regions.items():\n",
    "    for country in countries:\n",
    "        data.append({\"Country\": country, \"Region\": region})\n",
    "\n",
    "# Convert the list into a pandas DataFrame\n",
    "df_country_regions = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e54b9a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_area_countries = pd.read_csv(os.path.join(source_data_path,'area_by_country.csv'))\n",
    "surface_area_countries.drop(columns=['COUNTRY'], inplace=True)\n",
    "\n",
    "# Select the row with the highest area for each ISO3 (duplicates for islands)\n",
    "surface_area_countries = surface_area_countries.loc[\n",
    "    surface_area_countries.groupby('ISO3')['area_sq_km'].idxmax()\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "741fb4ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "milk_animals=pd.read_csv(os.path.join(source_data_path,'cattle','pop_cattle_dairy_cattle.csv'))\n",
    "milk_animals=milk_animals.loc[:,['Element','Area','Value','Year','ISO3']]\n",
    "milk_animals.pivot(index=['Area','Year'],columns=['Element'],values=['Value']).reset_index()\n",
    "milk_animals.drop(columns=['Element'],inplace=True)\n",
    "milk_animals.columns=['Area','Milk Animals','Year','ISO3']\n",
    "milk_animals.rename(columns={'Value':'Milk Animals','Area':'Country'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75329027",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_cows=pd.read_csv(os.path.join(source_data_path,'cattle','cattle_pop_2024.csv'))\n",
    "tot_cows=tot_cows.loc[:,['Area','Value','Year','ISO3']]\n",
    "tot_cows.rename(columns={'Value':'Total cattle','Area':'Country'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "218d545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "egg_animals=pd.read_csv(os.path.join(source_data_path,'poultry','pop_egg_layers.csv'))\n",
    "egg_animals=egg_animals.loc[:,['Element','Area','Value','Year','ISO3']]\n",
    "egg_animals.groupby(['Area','Year']).sum().reset_index()\n",
    "egg_animals['Value']*=1000\n",
    "egg_animals.rename(columns={'Value':'Laying Animals','Area':'Country'},inplace=True)\n",
    "egg_animals.drop(columns=['Element'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "187fcad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_chickens=pd.read_csv(os.path.join(source_data_path,'poultry','poultry_pop_2024.csv'))\n",
    "tot_chickens=tot_chickens.loc[:,['Area','Value','Year','Item','ISO3']]\n",
    "tot_chickens=tot_chickens[tot_chickens['Item']=='Chickens']\n",
    "tot_chickens['Value']*=1000\n",
    "tot_chickens.drop(columns=['Item'],inplace=True)\n",
    "tot_chickens.rename(columns={'Value':'Total chickens','Area':'Country'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6204ed9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_poultry=pd.read_csv(os.path.join(source_data_path,'poultry','poultry_pop_2024.csv'))\n",
    "tot_poultry = tot_poultry.sort_values('Value').drop_duplicates(subset=['ISO3','Year','Item'], keep='last')\n",
    "tot_poultry = (\n",
    "    tot_poultry.groupby(['ISO3', 'Year Code'], as_index=False)\n",
    "    .agg({\n",
    "        'Domain Code': 'first',\n",
    "        'Domain': 'first',\n",
    "        'Area Code (M49)': 'first',\n",
    "        'Area': 'first',\n",
    "        'Element Code': 'first',\n",
    "        'Element': 'first',\n",
    "        'Item Code (CPC)': 'first',\n",
    "        'Year Code': 'first',\n",
    "        'Year': 'first',\n",
    "        'Unit': 'first',\n",
    "        'Value': 'sum',  # Aggregate the 'Value' by summing\n",
    "        'Flag': 'first',\n",
    "        'Flag Description': 'first',\n",
    "        'Note': 'first',\n",
    "        'ISO3':'first'\n",
    "    })\n",
    ")\n",
    "# Set the 'Item' column to 'Poultry'\n",
    "tot_poultry=tot_poultry.sort_values('Year').loc[:,['Area','Value','ISO3','Year']]\n",
    "tot_poultry['Value']*=1000\n",
    "tot_poultry.rename(columns={'Value':'Total poultry','Area':'Country'},inplace=True)\n",
    "\n",
    "tot_pigs=pd.read_csv(os.path.join(source_data_path,'swine','swine_pop_2024.csv'))\n",
    "tot_pigs=tot_pigs.loc[:,['Area','Value','Year','Item','ISO3']]\n",
    "tot_pigs.drop(columns=['Item'],inplace=True)\n",
    "tot_pigs.rename(columns={'Value':'Total pigs','Area':'Country'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ceb99f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = {}\n",
    "for country in pycountry.countries:\n",
    "    countries[country.name] = country.alpha_3\n",
    "    countries['USA']='USA'\n",
    "    countries['UK']='GBR'\n",
    "    countries['Taiwan']='TWN'\n",
    "    countries['South Korea']='KOR'\n",
    "    countries['Czech Republic']='CZE'\n",
    "    countries['Brunei']='BRN'\n",
    "    countries['Russia']='RUS'\n",
    "    countries['Iran']='IRN'\n",
    "    countries['United States of America']='USA'\n",
    "    countries['Venezuela']='VEN'\n",
    "    countries['China (Hong Kong SAR)']='HKG'\n",
    "    countries[\"Cote d'Ivoire\"]='CIV'\n",
    "    countries['DR Congo']='COD'\n",
    "    countries['Guinea Bissau']='GNB'\n",
    "    countries['Lao PDR']='LAO'\n",
    "    countries['Micronesia (Federated States of)']='FSM'\n",
    "    countries['North Korea']='PRK'\n",
    "    countries['Occupied Palestinian Territory']='PSE'\n",
    "    countries['Swaziland']='SWZ'\n",
    "    countries['Tanzania']='TZA'\n",
    "    countries['Bolivia']='BOL'\n",
    "    countries['Macedonia (TFYR)']='MKD'\n",
    "    countries['Moldova']='MDA'\n",
    "    countries['Bolivia (Plurinational State of)']='BOL'\n",
    "    countries['China, Hong Kong SAR']='HKG'\n",
    "    countries['China, Taiwan Province of']='TWN'\n",
    "    countries['China, mainland']='CHN'\n",
    "    countries['Czechoslovakia']='CSK'\n",
    "    countries[\"Democratic People's Republic of Korea\"]='PRK'\n",
    "    countries['Democratic Republic of the Congo']='COD'\n",
    "    countries['French Guyana']='GUF'\n",
    "    countries['Micronesia']='FSM'\n",
    "    countries['Palestine']='PSE'\n",
    "    countries['Polynesia']='PYF'\n",
    "    countries['Republic of Korea']='KOR'\n",
    "    countries['Serbia and Montenegro']='SCG'\n",
    "    countries['Sudan (former)']='SDN'\n",
    "    countries['Türkiye']='TUR'\n",
    "    countries['USSR']='SUN'\n",
    "    countries['Iran (Islamic Republic of)']='IRN'\n",
    "    countries['Republic of Moldova']='MDA'\n",
    "    countries['United Kingdom of Great Britain and Northern Ireland']='GBR'\n",
    "    countries['United Republic of Tanzania']='TZA'\n",
    "    countries['Venezuela (Bolivarian Republic of)']='VEN'\n",
    "    countries['Yugoslav SFR']='YUG'\n",
    "    countries['Ethiopia PDR']='ETH'\n",
    "    countries['Central African (Rep.)']='CAF'\n",
    "    countries[\"China (People's Rep. of)\"]='CHN'\n",
    "    countries['Chinese Taipei']='TWN'\n",
    "    countries['Congo (Dem. Rep. of the)']='COD'\n",
    "    countries['Congo (Rep. of the)']='COG'\n",
    "    countries[\"Cote D'Ivoire\"]='CIV'\n",
    "    countries['Dominican (Rep.)']='DOM'\n",
    "    countries[\"Korea (Dem People's Rep. of)\"]='PRK'\n",
    "    countries['Korea (Rep. of)']='KOR'\n",
    "    countries['Laos']='LAO'\n",
    "    countries['South Sudan (Rep. of)']='SSD'\n",
    "    countries['Syria']='SYR'\n",
    "    countries['St. Vincent and the Grenadines']='VCT'\n",
    "    countries['Vietnam']='VNM'\n",
    "    countries['Reunion']='REU'\n",
    "    countries['Guadaloupe']='GLP'\n",
    "    countries['China, Macao SAR']='MAC'\n",
    "    countries['Netherlands (Kingdom of the)']='NLD'\n",
    "    countries['Türkiye (Rep. of)']='TUR'\n",
    "    countries['Belgium-Luxembourg']=\"BLX\"\n",
    "    countries['Kosovo']='XKX'\n",
    "\n",
    "\n",
    "    \n",
    "return_country = {v: k for k, v in countries.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13d2f612",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AFG' 'AGO' 'ALB' 'ARE' 'ARG' 'ARM' 'ATG' 'AUS' 'AUT' 'AZE' 'BDI' 'BEL'\n",
      " 'BEN' 'BFA' 'BGD' 'BGR' 'BHR' 'BHS' 'BIH' 'BLR' 'BLX' 'BLZ' 'BOL' 'BRA'\n",
      " 'BRB' 'BRN' 'BTN' 'BWA' 'CAF' 'CAN' 'CHE' 'CHL' 'CHN' 'CIV' 'CMR' 'COD'\n",
      " 'COG' 'COL' 'COM' 'CPV' 'CRI' 'CSK' 'CUB' 'CYP' 'CZE' 'DEU' 'DJI' 'DMA'\n",
      " 'DNK' 'DOM' 'DZA' 'ECU' 'EGY' 'ERI' 'ESP' 'EST' 'ETH' 'FIN' 'FJI' 'FRA'\n",
      " 'FRO' 'GAB' 'GBR' 'GEO' 'GHA' 'GIN' 'GLP' 'GMB' 'GNB' 'GRC' 'GRD' 'GTM'\n",
      " 'GUF' 'GUY' 'HKG' 'HND' 'HRV' 'HTI' 'HUN' 'IDN' 'IND' 'IRL' 'IRN' 'IRQ'\n",
      " 'ISL' 'ISR' 'ITA' 'JAM' 'JOR' 'JPN' 'KAZ' 'KEN' 'KGZ' 'KHM' 'KOR' 'KWT'\n",
      " 'LAO' 'LBN' 'LBR' 'LBY' 'LCA' 'LKA' 'LSO' 'LTU' 'LUX' 'LVA' 'MAR' 'MDA'\n",
      " 'MDG' 'MEX' 'MKD' 'MLI' 'MLT' 'MMR' 'MNE' 'MNG' 'MOZ' 'MRT' 'MTQ' 'MUS'\n",
      " 'MWI' 'MYS' 'NAM' 'NCL' 'NER' 'NGA' 'NIC' 'NIU' 'NLD' 'NOR' 'NPL' 'NZL'\n",
      " 'OMN' 'PAK' 'PAN' 'PER' 'PHL' 'PNG' 'POL' 'PRI' 'PRK' 'PRT' 'PRY' 'PSE'\n",
      " 'PYF' 'QAT' 'REU' 'ROU' 'RUS' 'RWA' 'SAU' 'SCG' 'SDN' 'SEN' 'SLB' 'SLE'\n",
      " 'SLV' 'SOM' 'SRB' 'SSD' 'STP' 'SUN' 'SUR' 'SVK' 'SVN' 'SWE' 'SWZ' 'SYC'\n",
      " 'SYR' 'TCD' 'TGO' 'THA' 'TJK' 'TKM' 'TON' 'TTO' 'TUN' 'TUR' 'TWN' 'TZA'\n",
      " 'UGA' 'UKR' 'URY' 'USA' 'UZB' 'VCT' 'VEN' 'VNM' 'VUT' 'WSM' 'YEM' 'YUG'\n",
      " 'ZAF' 'ZMB' 'ZWE']\n"
     ]
    }
   ],
   "source": [
    "pop_milk = milk_animals['Country']\n",
    "pop_tot_cow=tot_cows['Country']\n",
    "\n",
    "codes_milk = [countries.get(country, 'Unknown code:'+country) for country in pop_milk]\n",
    "codes_tot_cow = [countries.get(country, 'Unknown code:'+country) for country in pop_tot_cow]\n",
    "\n",
    "pop_laying = egg_animals['Country']\n",
    "pop_tot_chicken=tot_chickens['Country']\n",
    "\n",
    "codes_laying = [countries.get(country, 'Unknown code:'+country) for country in pop_laying]\n",
    "codes_tot_chicken = [countries.get(country, 'Unknown code:'+country) for country in pop_tot_chicken]\n",
    "\n",
    "country_regions_iso3=df_country_regions['Country']\n",
    "codes_regions=[countries.get(country, 'Unknown code:'+country) for country in country_regions_iso3]\n",
    "\n",
    "iso3s_milk=[]\n",
    "\n",
    "for i in pop_milk:\n",
    "    try:\n",
    "        iso3s_milk+=[countries[i]]\n",
    "    except:\n",
    "        iso3s_milk+=[None]\n",
    "        \n",
    "        \n",
    "iso3s_value_cow=[]\n",
    "\n",
    "for i in pop_tot_cow:\n",
    "    try:\n",
    "        iso3s_value_cow+=[countries[i]]\n",
    "    except:\n",
    "        iso3s_value_cow+=[None]\n",
    "        \n",
    "milk_animals['ISO3']=iso3s_milk\n",
    "tot_cows['ISO3']=iso3s_value_cow\n",
    "\n",
    "iso3s_egg=[]\n",
    "\n",
    "for i in pop_laying:\n",
    "    try:\n",
    "        iso3s_egg+=[countries[i]]\n",
    "    except:\n",
    "        iso3s_egg+=[None]\n",
    "        \n",
    "        \n",
    "iso3s_value_chicken=[]\n",
    "\n",
    "for i in pop_tot_chicken:\n",
    "    try:\n",
    "        iso3s_value_chicken+=[countries[i]]\n",
    "    except:\n",
    "        iso3s_value_chicken+=[None]\n",
    "        \n",
    "        \n",
    "iso3s_regions=[]\n",
    "\n",
    "for i in country_regions_iso3:\n",
    "    try:\n",
    "        iso3s_regions+=[countries[i]]\n",
    "    except:\n",
    "        iso3s_regions+=[None]\n",
    "        \n",
    "        \n",
    "df_country_regions['ISO3']=iso3s_regions\n",
    "        \n",
    "egg_animals['ISO3']=iso3s_egg\n",
    "tot_chickens['ISO3']=iso3s_value_chicken\n",
    "        \n",
    "print(np.unique(codes_milk))\n",
    "\n",
    "milk_animals=milk_animals.sort_values('Milk Animals').drop_duplicates(['ISO3','Year'],keep='last')\n",
    "tot_cows=tot_cows.sort_values('Total cattle').drop_duplicates(['ISO3','Year'],keep='last')\n",
    "\n",
    "egg_animals=egg_animals.sort_values('Laying Animals').drop_duplicates(['ISO3','Year'],keep='last')\n",
    "tot_chickens=tot_chickens.sort_values('Total chickens').drop_duplicates(['ISO3','Year'],keep='last')\n",
    "\n",
    "tot_pigs=tot_pigs.sort_values('Total pigs').drop_duplicates(['ISO3','Year'],keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d8b3d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "milk_animals=pd.merge(milk_animals.drop(columns=['Country']),tot_cows.drop(columns=['Country']),how='left',on=['ISO3','Year'])\n",
    "milk_animals['prop_milk_cows']=milk_animals['Milk Animals']/milk_animals['Total cattle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bb0dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "egg_animals=pd.merge(egg_animals.drop(columns=['Country']),tot_chickens.drop(columns=['Country']),how='left',on=['ISO3','Year'])\n",
    "egg_animals['prop_laying_chickens']=egg_animals['Laying Animals']/egg_animals['Total chickens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ee26efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf or -inf values are located at these (row, column) positions:\n"
     ]
    }
   ],
   "source": [
    "data_interest=egg_animals.drop(columns=['ISO3'])\n",
    "\n",
    "# Check for inf and -inf values in the DataFrame (X_train)\n",
    "mask = np.isinf(data_interest)\n",
    "\n",
    "# Show only rows and columns where inf/-inf values are located\n",
    "inf_locations = data_interest[mask]\n",
    "\n",
    "# Output the locations of the inf/-inf values (row, column)\n",
    "rows, cols = np.where(mask)\n",
    "\n",
    "# Print the row and column indices of the inf/-inf values\n",
    "print(\"inf or -inf values are located at these (row, column) positions:\")\n",
    "for row, col in zip(rows, cols):\n",
    "    print(f\"Row {row}, Column {data_interest.columns[col]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3c3e711",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjusting a few cases where the number of egg-laying animals is wrong If they exist (if exist, is very small number, less than 10, maybe less than 5):\n",
    "egg_animals.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b2cee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp=pd.read_csv(os.path.join(source_data_path,'GDP_per_capita.csv')).loc[:,['Country Code']+[str(i) for i in range(1960,2024,1)]]\n",
    "gdp.rename(columns={'Country Code':'ISO3'},inplace=True)\n",
    "gdp=gdp.melt(id_vars=['ISO3'],value_vars=[str(i) for i in range(1960,2024,1)])\n",
    "gdp.rename(columns={'variable':'Year','value':'GDP per Capita'},inplace=True)\n",
    "gdp=gdp[gdp['Year'].isin([str(i) for i in range(2005,2025,1)])]\n",
    "gdp['Year']=gdp['Year'].astype(float)\n",
    "#gdp=gdp[gdp['Year']==2023].drop(columns=['Year'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf70796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_pigs['total pigs']=tot_pigs['Total pigs']\n",
    "tot_poultry['total poultry']=tot_poultry['Total poultry']\n",
    "tot_cows['total cattle']=tot_cows['Total cattle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc8c4765",
   "metadata": {},
   "outputs": [],
   "source": [
    "cattle_vaccine=pd.read_csv(os.path.join(notebook_dir,'Vaccination Coverage','2005-2024_full_cattle_vaccine_coverage_by_country.csv'))\n",
    "cattle_vaccine.loc[cattle_vaccine['Country'] == 'Türkiye (Rep. of)', 'ISO3'] = 'TUR'\n",
    "cattle_vaccine=cattle_vaccine.loc[:,['ISO3','Country','Year','Disease','Vaccine Coverage']]\n",
    "#cattle=cattle.loc[:,['ISO3','Year','Disease','Vaccine Coverage']]\n",
    "cattle_vaccine=cattle_vaccine.pivot(index=['ISO3','Country','Year'],columns=['Disease'],values=['Vaccine Coverage'])\n",
    "cattle_vaccine.columns=[i[1]+'_cattle_vaccine_cov' for i in cattle_vaccine.columns]\n",
    "\n",
    "swine_vaccine=pd.read_csv(os.path.join(notebook_dir,'Vaccination Coverage','2005-2024_full_swine_vaccine_coverage_by_country.csv'))\n",
    "swine_vaccine.loc[swine_vaccine['Country'] == 'Türkiye (Rep. of)', 'ISO3'] = 'TUR'\n",
    "swine_vaccine=swine_vaccine.loc[:,['ISO3','Disease','Year','Vaccine Coverage']]\n",
    "swine_vaccine=swine_vaccine.pivot(index=['ISO3','Year'],columns=['Disease'],values=['Vaccine Coverage'])\n",
    "swine_vaccine.columns=[i[1]+'_swine_vaccine_cov' for i in swine_vaccine.columns]\n",
    "\n",
    "\n",
    "\n",
    "poultry_vaccine=pd.read_csv(os.path.join(notebook_dir,'Vaccination Coverage','2005-2024_full_poultry_vaccine_coverage_by_country.csv'))\n",
    "poultry_vaccine.loc[poultry_vaccine['Country'] == 'Türkiye (Rep. of)', 'ISO3'] = 'TUR'\n",
    "poultry_vaccine=poultry_vaccine.loc[:,['ISO3','Year','Disease','Vaccine Coverage']]\n",
    "\n",
    "poultry_vaccine=poultry_vaccine.pivot(index=['ISO3','Year'],columns=['Disease'],values=['Vaccine Coverage'])\n",
    "poultry_vaccine.columns=[i[1]+'_poultry_vaccine_cov' for i in poultry_vaccine.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e516326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cattle_incidence=pd.read_csv(os.path.join(notebook_dir,'Disease Incidence','2005-2024_cattle_disease_incidence_stats.csv'))\n",
    "cattle_incidence=cattle_incidence[cattle_incidence['Incidence Lower'].notna()]#Remove datapoints that could not have confidence interval generated\n",
    "cattle_incidence=cattle_incidence.loc[:,['ISO3','Country','Year','Disease','Incidence']]\n",
    "# Will be reporting cases per 100,000; incidence is currently = cases per animal\n",
    "cattle_incidence['Incidence']*=100000\n",
    "cattle_incidence=cattle_incidence.pivot(index=['ISO3','Country','Year'],columns=['Disease'],values=['Incidence'])\n",
    "cattle_incidence.columns=[i[1]+'_cattle_incidence' for i in cattle_incidence.columns]\n",
    "\n",
    "\n",
    "swine_incidence=pd.read_csv(os.path.join(notebook_dir,'Disease Incidence','2005-2024_swine_disease_incidence_stats.csv'))\n",
    "swine_incidence=swine_incidence[swine_incidence['Incidence Lower'].notna()]#Remove datapoints that could not have confidence interval generated\n",
    "swine_incidence=swine_incidence.loc[:,['ISO3','Disease','Year','Incidence']]\n",
    "# Will be reporting cases per 100,000; incidence is currently = cases per animal\n",
    "swine_incidence['Incidence']*=100000\n",
    "swine_incidence=swine_incidence.pivot(index=['ISO3','Year'],columns=['Disease'],values=['Incidence'])\n",
    "swine_incidence.columns=[i[1]+'_swine_incidence' for i in swine_incidence.columns]\n",
    "poultry_incidence=pd.read_csv(os.path.join(notebook_dir,'Disease Incidence','2005-2024_poultry_disease_incidence_stats.csv'))\n",
    "poultry_incidence=poultry_incidence[poultry_incidence['Incidence Lower'].notna()]#Remove datapoints that could not have confidence interval generated\n",
    "poultry_incidence=poultry_incidence.loc[:,['ISO3','Disease','Year','Incidence']]\n",
    "# Will be reporting cases per 100,000; incidence is currently = cases per animal\n",
    "poultry_incidence['Incidence']*=100000\n",
    "poultry_incidence=poultry_incidence.pivot(index=['ISO3','Year'],columns=['Disease'],values=['Incidence'])\n",
    "poultry_incidence.columns=[i[1]+'_poultry_incidence' for i in poultry_incidence.columns]\n",
    "\n",
    "\n",
    "df_impute_original=reduce(lambda  left,right: pd.merge(left,right,on=['ISO3','Year'],\n",
    "                                            how='outer'), [cattle_incidence,swine_incidence,poultry_incidence,\n",
    "                                                           cattle_vaccine,swine_vaccine,poultry_vaccine,\n",
    "                                                           gdp,milk_animals,egg_animals,\n",
    "                                                          tot_pigs.drop(columns=['Country']),\n",
    "                                                           tot_poultry.drop(columns=['Country','Total poultry']),\n",
    "                                                           tot_cows.drop(columns=['Country','Total cattle'])])\n",
    "\n",
    "df_impute_original=df_impute_original.merge(surface_area_countries,how='left',on=['ISO3'])\n",
    "\n",
    "df_impute_original=pd.merge(df_impute_original,df_country_regions,on='ISO3',how='left')\n",
    "df_impute_original=df_impute_original.drop(columns=['Milk Animals','Total cattle', 'Laying Animals','Total chickens', 'Total pigs'])\n",
    "\n",
    "df_impute_original=df_impute_original[(df_impute_original['Year']>=2005)&(df_impute_original['Year']<=2024)]\n",
    "\n",
    "drop_these_iso3s=np.unique(df_impute_original[df_impute_original['Country'].isna()]['ISO3'])\n",
    "df_impute_original = df_impute_original[~df_impute_original['ISO3'].isin(drop_these_iso3s)]\n",
    "\n",
    "df_impute_original['pig_density']=df_impute_original['total pigs']/df_impute_original['area_sq_km']\n",
    "df_impute_original['poultry_density']=df_impute_original['total poultry']/df_impute_original['area_sq_km']\n",
    "df_impute_original['cattle_density']=df_impute_original['total cattle']/df_impute_original['area_sq_km']\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit and transform the 'Region' column to integers\n",
    "df_impute_original['Region_encoded'] = le.fit_transform(df_impute_original['Region'])\n",
    "df_impute_original.drop(columns=['Region','Country'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54f86622",
   "metadata": {},
   "outputs": [],
   "source": [
    "cattle_vaccine=pd.read_csv(os.path.join(notebook_dir,'Vaccination Coverage','2005-2024_full_cattle_vaccine_coverage_by_country.csv'))\n",
    "cattle_vaccine.loc[cattle_vaccine['Country'] == 'Türkiye (Rep. of)', 'ISO3'] = 'TUR'\n",
    "cattle_vaccine=cattle_vaccine.loc[:,['ISO3','Country','Year','Disease','Vaccine Coverage','Vaccine Coverage Lower','Vaccine Coverage Upper']]\n",
    "cattle_vaccine.drop_duplicates(inplace=True)\n",
    "cattle_vaccine=cattle_vaccine.pivot(index=['ISO3','Year'],columns=['Disease'],values=['Vaccine Coverage','Vaccine Coverage Lower','Vaccine Coverage Upper'])\n",
    "#Rename columns to include the naming convention\n",
    "cattle_vaccine.columns = [f\"{disease}_cattle_vaccine_cov\" if var == 'Vaccine Coverage' else \n",
    "                   f\"{disease}_cattle_vaccine_cov_lower\" if var == 'Vaccine Coverage Lower' else \n",
    "                   f\"{disease}_cattle_vaccine_cov_upper\" \n",
    "                   for var, disease in cattle_vaccine.columns]\n",
    "\n",
    "swine_vaccine=pd.read_csv(os.path.join(notebook_dir,'Vaccination Coverage','2005-2024_full_swine_vaccine_coverage_by_country.csv'))\n",
    "swine_vaccine.loc[swine_vaccine['Country'] == 'Türkiye (Rep. of)', 'ISO3'] = 'TUR'\n",
    "swine_vaccine=swine_vaccine.loc[:,['ISO3','Disease','Year','Vaccine Coverage','Vaccine Coverage Lower','Vaccine Coverage Upper']]\n",
    "swine_vaccine.drop_duplicates(inplace=True)\n",
    "swine_vaccine=swine_vaccine.pivot(index=['ISO3','Year'],columns=['Disease'],values=['Vaccine Coverage','Vaccine Coverage Lower','Vaccine Coverage Upper'])\n",
    "#Rename columns to include the naming convention\n",
    "swine_vaccine.columns = [f\"{disease}_swine_vaccine_cov\" if var == 'Vaccine Coverage' else \n",
    "                   f\"{disease}_swine_vaccine_cov_lower\" if var == 'Vaccine Coverage Lower' else \n",
    "                   f\"{disease}_swine_vaccine_cov_upper\" \n",
    "                   for var, disease in swine_vaccine.columns]\n",
    "\n",
    "\n",
    "poultry_vaccine=pd.read_csv(os.path.join(notebook_dir,'Vaccination Coverage','2005-2024_full_poultry_vaccine_coverage_by_country.csv'))\n",
    "poultry_vaccine.loc[poultry_vaccine['Country'] == 'Türkiye (Rep. of)', 'ISO3'] = 'TUR'\n",
    "poultry_vaccine = poultry_vaccine.loc[:, ['ISO3', 'Year', 'Disease', 'Vaccine Coverage', 'Vaccine Coverage Lower', 'Vaccine Coverage Upper']]\n",
    "poultry_vaccine.drop_duplicates(inplace=True)\n",
    "\n",
    "# Pivot table with new column naming convention\n",
    "poultry_vaccine = poultry_vaccine.pivot(index=['ISO3', 'Year'], columns=['Disease'], values=['Vaccine Coverage', 'Vaccine Coverage Lower', 'Vaccine Coverage Upper'])\n",
    "\n",
    "#Rename columns to include the naming convention\n",
    "poultry_vaccine.columns = [f\"{disease}_poultry_vaccine_cov\" if var == 'Vaccine Coverage' else \n",
    "                   f\"{disease}_poultry_vaccine_cov_lower\" if var == 'Vaccine Coverage Lower' else \n",
    "                   f\"{disease}_poultry_vaccine_cov_upper\" \n",
    "                   for var, disease in poultry_vaccine.columns]\n",
    "\n",
    "poultry_vaccine.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e457cac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cattle_incidence=pd.read_csv(os.path.join(notebook_dir,'Disease Incidence','2005-2024_cattle_disease_incidence_stats.csv'))\n",
    "cattle_incidence=cattle_incidence.loc[:,['ISO3','Country','Year','Disease','Incidence','Incidence Lower','Incidence Upper']]\n",
    "# Will be reporting cases per 100,000; incidence is currently = cases per animal\n",
    "cattle_incidence['Incidence']*=100000\n",
    "cattle_incidence['Incidence Lower']*=100000\n",
    "cattle_incidence['Incidence Upper']*=100000\n",
    "cattle_incidence=cattle_incidence[cattle_incidence['Incidence Lower'].notna()]#Remove datapoints that could not have confidence interval generated\n",
    "cattle_incidence=cattle_incidence.pivot(index=['ISO3','Year'],columns=['Disease'],values=['Incidence','Incidence Lower','Incidence Upper'])\n",
    "# Rename columns to include the naming convention\n",
    "cattle_incidence.columns = [f\"{disease}_cattle_incidence\" if var == 'Incidence' else \n",
    "                   f\"{disease}_cattle_incidence_lower\" if var == 'Incidence Lower' else \n",
    "                   f\"{disease}_cattle_incidence_upper\" \n",
    "                   for var, disease in cattle_incidence.columns]\n",
    "\n",
    "\n",
    "swine_incidence=pd.read_csv(os.path.join(notebook_dir,'Disease Incidence','2005-2024_swine_disease_incidence_stats.csv'))\n",
    "swine_incidence=swine_incidence.loc[:,['ISO3','Disease','Year','Incidence','Incidence Lower','Incidence Upper']]\n",
    "# Will be reporting cases per 100,000; incidence is currently = cases per animal\n",
    "swine_incidence['Incidence']*=100000\n",
    "swine_incidence['Incidence Lower']*=100000\n",
    "swine_incidence['Incidence Upper']*=100000\n",
    "swine_incidence=swine_incidence[swine_incidence['Incidence Lower'].notna()]#Remove datapoints that could not have confidence interval generated\n",
    "swine_incidence=swine_incidence.pivot(index=['ISO3','Year'],columns=['Disease'],values=['Incidence','Incidence Lower','Incidence Upper'])\n",
    "# Rename columns to include the naming convention\n",
    "swine_incidence.columns = [f\"{disease}_swine_incidence\" if var == 'Incidence' else \n",
    "                   f\"{disease}_swine_incidence_lower\" if var == 'Incidence Lower' else \n",
    "                   f\"{disease}_swine_incidence_upper\" \n",
    "                   for var, disease in swine_incidence.columns]\n",
    "\n",
    "poultry_incidence=pd.read_csv(os.path.join(notebook_dir,'Disease Incidence','2005-2024_poultry_disease_incidence_stats.csv'))\n",
    "poultry_incidence.loc[poultry_incidence['Country'] == 'Türkiye (Rep. of)', 'ISO3'] = 'TUR'\n",
    "poultry_incidence = poultry_incidence.loc[:, ['ISO3', 'Year', 'Disease', 'Incidence', 'Incidence Lower', 'Incidence Upper']]\n",
    "\n",
    "# Will be reporting cases per 100,000; incidence is currently = cases per animal\n",
    "poultry_incidence['Incidence']*=100000\n",
    "poultry_incidence['Incidence Lower']*=100000\n",
    "poultry_incidence['Incidence Upper']*=100000\n",
    "poultry_incidence=poultry_incidence[poultry_incidence['Incidence Lower'].notna()]#Remove datapoints that could not have confidence interval generated\n",
    "# Pivot table with new column naming convention\n",
    "poultry_incidence = poultry_incidence.pivot(index=['ISO3', 'Year'], columns=['Disease'], values=['Incidence', 'Incidence Lower', 'Incidence Upper'])\n",
    "\n",
    "# Rename columns to include the naming convention\n",
    "poultry_incidence.columns = [f\"{disease}_poultry_incidence\" if var == 'Incidence' else \n",
    "                   f\"{disease}_poultry_incidence_lower\" if var == 'Incidence Lower' else \n",
    "                   f\"{disease}_poultry_incidence_upper\" \n",
    "                   for var, disease in poultry_incidence.columns]\n",
    "\n",
    "poultry_incidence.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "df_impute_sampling=reduce(lambda  left,right: pd.merge(left,right,on=['ISO3','Year'],\n",
    "                                            how='outer'), [cattle_incidence,swine_incidence,poultry_incidence,\n",
    "                                                           cattle_vaccine,swine_vaccine,poultry_vaccine,\n",
    "                                                           gdp,milk_animals,egg_animals,\n",
    "                                                          tot_pigs.drop(columns=['Country']),\n",
    "                                                           tot_poultry.drop(columns=['Country','Total poultry']),\n",
    "                                                           tot_cows.drop(columns=['Country','Total cattle'])])\n",
    "\n",
    "df_impute_sampling=df_impute_sampling.merge(surface_area_countries,how='left',on=['ISO3'])\n",
    "\n",
    "df_impute_sampling=df_impute_sampling.drop(columns=['Milk Animals','Total cattle', 'Laying Animals','Total chickens','Total pigs'])\n",
    "\n",
    "df_impute_sampling=df_impute_sampling[(df_impute_sampling['Year']>=2005)&(df_impute_sampling['Year']<=2024)]\n",
    "df_impute_sampling = df_impute_sampling[~df_impute_sampling['ISO3'].isin(drop_these_iso3s)]\n",
    "df_impute_sampling=pd.merge(df_impute_sampling,df_country_regions,on='ISO3',how='left')\n",
    "\n",
    "df_impute_sampling['pig_density']=df_impute_sampling['total pigs']/df_impute_sampling['area_sq_km']\n",
    "df_impute_sampling['poultry_density']=df_impute_sampling['total poultry']/df_impute_sampling['area_sq_km']\n",
    "df_impute_sampling['cattle_density']=df_impute_sampling['total cattle']/df_impute_sampling['area_sq_km']\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit and transform the 'Region' column to integers\n",
    "df_impute_sampling['Region_encoded'] = le.fit_transform(df_impute_sampling['Region'])\n",
    "df_impute_sampling.drop(columns=['Region'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a599c2b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique ISO3 values of outlier countries:\n",
      "['BWA' 'TUR']\n",
      "Max values by ISO3:\n",
      "ISO3\n",
      "BWA    365.038723\n",
      "TUR    105.485232\n",
      "Name: Rabies virus (Inf. with)_swine_incidence, dtype: float64\n",
      "Percent of rows removed: 0.005896%\n"
     ]
    }
   ],
   "source": [
    "total_rows_removed = 0\n",
    "total_rows_processed = 0\n",
    "\n",
    "for col in df_impute_original.columns:\n",
    "    if ('_swine_incidence' in col ) or ('_cattle_incidence' in col) or ('_poultry_incidence' in col):\n",
    "        \n",
    "        if '_swine' in col:\n",
    "            pop_size_col='total pigs'\n",
    "        elif '_cattle' in col:\n",
    "            pop_size_col='total cattle'\n",
    "        else:\n",
    "            pop_size_col='total poultry'\n",
    "\n",
    "\n",
    "        disease_col = col\n",
    "        data = df_impute_original.copy()\n",
    "        iso3_col = 'ISO3'\n",
    "        min_pop_threshold = 50000\n",
    "\n",
    "        max_incidence_per_country = data.groupby(iso3_col)[disease_col].max()\n",
    "\n",
    "        epsilon = 1e-9 # Small constant to avoid log(0)\n",
    "        max_incidence_per_country_log = np.log10(max_incidence_per_country + epsilon)\n",
    "\n",
    "        #Computing thresholds for high outliers using Modified Z-Score\n",
    "        median_log_max_incidence = max_incidence_per_country_log.median()\n",
    "        mad_log_max_incidence = np.median(np.abs(\n",
    "            np.array([i for i in max_incidence_per_country_log.values if i == i]) - median_log_max_incidence\n",
    "        ))\n",
    "        outlier_threshold = 3.5\n",
    "\n",
    "        # Identifying countries with outlier maximum incidence values\n",
    "        outlier_countries = max_incidence_per_country_log[\n",
    "            0.6745 * (max_incidence_per_country_log - median_log_max_incidence) / mad_log_max_incidence > outlier_threshold\n",
    "        ].index\n",
    "\n",
    "        data[\"Too_Small_Pop\"] = data[pop_size_col] < min_pop_threshold\n",
    "\n",
    "        data[\"Is_Outlier_Country\"] = data[iso3_col].isin(outlier_countries)\n",
    "        filtered_data = data[~(data[\"Is_Outlier_Country\"] & data[\"Too_Small_Pop\"])]\n",
    "\n",
    "        rows_removed = len(data) - len(filtered_data)\n",
    "        total_rows_removed += rows_removed\n",
    "        total_rows_processed += len(data)\n",
    "        \n",
    "        if len(np.unique(data[data[\"Is_Outlier_Country\"]&data[\"Too_Small_Pop\"]]['ISO3']))>0:\n",
    "\n",
    "            print(\"Unique ISO3 values of outlier countries:\")\n",
    "            print(np.unique(data[data[\"Is_Outlier_Country\"]&data[\"Too_Small_Pop\"]]['ISO3']))\n",
    "\n",
    "            max_values_by_iso3 = data[data[\"Is_Outlier_Country\"]&data[\"Too_Small_Pop\"]].groupby('ISO3')[disease_col].max()\n",
    "            print(\"Max values by ISO3:\")\n",
    "            print(max_values_by_iso3)\n",
    "\n",
    "        filtered_data = filtered_data.drop(columns=[\"Too_Small_Pop\", \"Is_Outlier_Country\"], errors=\"ignore\")\n",
    "\n",
    "proportion_removed = total_rows_removed / total_rows_processed\n",
    "print(f\"Percent of rows removed: {proportion_removed*100:.6f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c1d42bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_outliers(data, disease_col, pop_size_col, iso3_col='ISO3', year_col='Year',\n",
    "                      min_pop_threshold=50000, outlier_threshold=3.5):#min_pop_threshold=120000, outlier_threshold=2.5):\n",
    "    \"\"\"\n",
    "    Identifies ISO3, Year pairs that meet the outlier criteria based on maximum disease incidence\n",
    "    and population size thresholds.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The input dataframe.\n",
    "        disease_col (str): The name of the disease incidence column.\n",
    "        pop_size_col (str): The name of the animal population size column.\n",
    "        iso3_col (str): The name of the ISO3 country code column.\n",
    "        year_col (str): The name of the year column.\n",
    "        min_pop_threshold (int, optional): The minimum population size threshold (default=50,000.\n",
    "        outlier_threshold (float, optional): The Modified Z-Score threshold for outliers (default=3.5)\n",
    "\n",
    "    Returns:\n",
    "        set: A set of tuples representing the identified (ISO3, Year) pairs.\n",
    "    \"\"\"\n",
    "    data = data.copy()  \n",
    "\n",
    "    #Calculate the maximum incidence per country (ISO3)\n",
    "    max_incidence_per_country = data.groupby(iso3_col)[disease_col].max()\n",
    "\n",
    "    # Log-transform the maximum incidence values\n",
    "    epsilon = 1e-9  # Small constant to avoid log(0)\n",
    "    max_incidence_per_country_log = np.log10(max_incidence_per_country + epsilon)\n",
    "\n",
    "    # ompute thresholds for high outliers using Modified Z-Score\n",
    "    median_log_max_incidence = max_incidence_per_country_log.median()\n",
    "    mad_log_max_incidence = np.median(\n",
    "        np.abs(np.array([i for i in max_incidence_per_country_log.values if i == i]) - median_log_max_incidence)\n",
    "    )\n",
    "\n",
    "    outlier_countries = max_incidence_per_country_log[\n",
    "        0.6745 * (max_incidence_per_country_log - median_log_max_incidence) / mad_log_max_incidence > outlier_threshold\n",
    "    ].index\n",
    "\n",
    "    # Apply the fixed minimum population size threshold\n",
    "    data[\"Too_Small_Pop\"] = data[pop_size_col] < min_pop_threshold\n",
    "    data[\"Is_Outlier_Country\"] = data[iso3_col].isin(outlier_countries)\n",
    "\n",
    "    # Identify rows to flag based on actual values in 'disease_col'\n",
    "    rows_to_flag = (\n",
    "        data[\"Is_Outlier_Country\"] & data[\"Too_Small_Pop\"] & data[disease_col].notna()\n",
    "    )\n",
    "\n",
    "    flagged_iso3_year_pairs = set(\n",
    "        data.loc[rows_to_flag, [iso3_col, year_col]].drop_duplicates().itertuples(index=False, name=None)\n",
    "    )\n",
    "\n",
    "    return flagged_iso3_year_pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0365ae2f",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3d0f8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip function to bound predictions between 0 and 1\n",
    "def clip_values(y_pred,col_name):\n",
    "    if 'incidence' in col_name:\n",
    "        return np.maximum(y_pred, 0)\n",
    "    else:\n",
    "        return np.clip(y_pred, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65d01ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_interest=[i for i in df_impute_original.columns if i not in ['ISO3','Year','GDP per Capita','prop_milk_cows','prop_laying_chickens',\n",
    "                                                   'Region_encoded']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "678f517a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "308"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(columns_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15547383",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Clear the optuna.log file\n",
    "    with open(\"optuna.log\", \"w\") as f:\n",
    "        pass  # Opening in 'w' mode clears the file\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Get Optuna logger\n",
    "optuna_logger = logging.getLogger(\"optuna\")\n",
    "\n",
    "optuna_logger.setLevel(logging.INFO)\n",
    "\n",
    "# Remove default console (stream) handler to prevent output in the notebook\n",
    "optuna_logger.handlers = []\n",
    "\n",
    "# Add a file handler to redirect logs to a file\n",
    "file_handler = logging.FileHandler(\"optuna.log\")\n",
    "optuna_logger.addHandler(file_handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94d3d6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pop_cattle=pd.read_csv(os.path.join(source_data_path, 'cattle','cattle_pop_2024.csv'))\n",
    "df_pop_cattle.rename(columns={'Value':'latest pop cattle'},inplace=True)\n",
    "df_pop_cattle=df_pop_cattle.sort_values('Year').drop_duplicates(['ISO3','Year'],keep='last').loc[:,['ISO3','latest pop cattle','Year']]\n",
    "df_pop_cattle['Animal']=['Cattle']*df_pop_cattle.shape[0]\n",
    "\n",
    "df_pop_poultry=pd.read_csv(os.path.join(source_data_path, 'poultry','poultry_pop_2024.csv'))\n",
    "df_pop_poultry = (\n",
    "    df_pop_poultry.groupby(['Area Code (M49)', 'Year Code'], as_index=False)\n",
    "    .agg({\n",
    "        'Domain Code': 'first',\n",
    "        'ISO3':'first',\n",
    "        'Domain': 'first',\n",
    "        'Area Code (M49)': 'first',\n",
    "        'Area': 'first',\n",
    "        'Element Code': 'first',\n",
    "        'Element': 'first',\n",
    "        'Item Code (CPC)': 'first',\n",
    "        'Year Code': 'first',\n",
    "        'Year': 'first',\n",
    "        'Unit': 'first',\n",
    "        'Value': 'sum',  \n",
    "        'Flag': 'first',\n",
    "        'Flag Description': 'first',\n",
    "        'Note': 'first'\n",
    "    })\n",
    ")\n",
    "df_pop_poultry['Item'] = 'Poultry'\n",
    "df_pop_poultry=df_pop_poultry.sort_values('Year').drop_duplicates(['ISO3','Year'],keep='last').loc[:,['ISO3','Value','Year']]\n",
    "df_pop_poultry.rename(columns={'Value':'latest pop poultry'},inplace=True)\n",
    "df_pop_poultry['Animal']=['Poultry']*df_pop_poultry.shape[0]\n",
    "df_pop_poultry['latest pop poultry']*=1000\n",
    "\n",
    "\n",
    "df_pop_swine=pd.read_csv(os.path.join(source_data_path, 'swine','swine_pop_2024.csv'))\n",
    "df_pop_swine.rename(columns={'Value':'latest pop swine'},inplace=True)\n",
    "df_pop_swine['Animal']=['Pigs']*df_pop_swine.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91e4c58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_killed_pop_cattle=pd.read_csv(os.path.join(source_data_path, 'cattle','killed_cattle_pop_2024.csv'))\n",
    "df_killed_pop_cattle.rename(columns={'Value':'killed pop cattle'},inplace=True)\n",
    "df_killed_pop_cattle=df_killed_pop_cattle.sort_values('Year').drop_duplicates(['ISO3','Year'],keep='last').loc[:,['ISO3','killed pop cattle','Year']]\n",
    "df_killed_pop_cattle['Animal']=['Cattle']*df_killed_pop_cattle.shape[0]\n",
    "\n",
    "\n",
    "df_killed_pop_poultry=pd.read_csv(os.path.join(source_data_path, 'poultry','killed_poultry_pop_2024.csv'))\n",
    "df_killed_pop_poultry = (\n",
    "    df_killed_pop_poultry.groupby(['Area Code (M49)', 'Year Code'], as_index=False)\n",
    "    .agg({\n",
    "        'Domain Code': 'first',\n",
    "        'Domain': 'first',\n",
    "        'Area Code (M49)': 'first',\n",
    "        'ISO3':'first',\n",
    "        'Area': 'first',\n",
    "        'Element Code': 'first',\n",
    "        'Element': 'first',\n",
    "        'Item Code (CPC)': 'first',\n",
    "        'Year Code': 'first',\n",
    "        'Year': 'first',\n",
    "        'Unit': 'first',\n",
    "        'Value': 'sum',  \n",
    "        'Flag': 'first',\n",
    "        'Flag Description': 'first',\n",
    "        'Note': 'first'\n",
    "    })\n",
    ")\n",
    "df_killed_pop_poultry['Item'] = 'Poultry'\n",
    "df_killed_pop_poultry=df_killed_pop_poultry.sort_values('Year').drop_duplicates(['ISO3','Year'],keep='last').loc[:,['ISO3','Value','Year']]\n",
    "df_killed_pop_poultry.rename(columns={'Value':'killed pop poultry'},inplace=True)\n",
    "df_killed_pop_poultry['Animal']=['Poultry']*df_killed_pop_poultry.shape[0]\n",
    "df_killed_pop_poultry['killed pop poultry']*=1000\n",
    "\n",
    "\n",
    "df_killed_pop_swine=pd.read_csv(os.path.join(source_data_path, 'swine','killed_swine_pop_2024.csv')).sort_values('Year').drop_duplicates(['ISO3','Year'],keep='last').loc[:,['ISO3','Value','Year']]\n",
    "df_killed_pop_swine.rename(columns={'Value':'killed pop swine'},inplace=True)\n",
    "df_killed_pop_swine['Animal']=['Pigs']*df_killed_pop_swine.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1aa06e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_impute_original=df_impute_original.merge(df_pop_cattle.drop(columns=['Animal']),how='left',on=['ISO3','Year'])\n",
    "df_impute_original=df_impute_original.merge(df_pop_swine.drop(columns=['Animal']),how='left',on=['ISO3','Year'])\n",
    "df_impute_original=df_impute_original.merge(df_pop_poultry.drop(columns=['Animal']),how='left',on=['ISO3','Year'])\n",
    "\n",
    "df_impute_original=df_impute_original.merge(df_killed_pop_cattle.drop(columns=['Animal']),how='left',on=['ISO3','Year'])\n",
    "df_impute_original=df_impute_original.merge(df_killed_pop_swine.drop(columns=['Animal']),how='left',on=['ISO3','Year'])\n",
    "df_impute_original=df_impute_original.merge(df_killed_pop_poultry.drop(columns=['Animal']),how='left',on=['ISO3','Year'])\n",
    "\n",
    "\n",
    "df_impute_original['total_cattle']=df_impute_original['latest pop cattle']+df_impute_original['killed pop cattle']\n",
    "df_impute_original['total_poultry']=df_impute_original['latest pop poultry']+df_impute_original['killed pop poultry']\n",
    "df_impute_original['total_swine']=df_impute_original['latest pop swine']+df_impute_original['killed pop swine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4a3cda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cattle_weights=np.nan_to_num(df_impute_original['total_cattle'], nan=0)\n",
    "total_poultry_weights=np.nan_to_num(df_impute_original['total_poultry'], nan=0)\n",
    "total_swine_weights=np.nan_to_num(df_impute_original['total_swine'], nan=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6760d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index of the column \"Region_encoded\"\n",
    "col_index = df_impute_original.columns.get_loc(\"Region_encoded\")\n",
    "\n",
    "# Keep only columns up to \"Region_encoded\" (inclusive)\n",
    "df_impute_original = df_impute_original.iloc[:, :col_index + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60532d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, X_train, y_train, countries, feature_importance, mandatory_features):\n",
    "    # Map each row to its corresponding country\n",
    "    unique_countries = pd.Series(countries).unique()\n",
    "\n",
    "    # Define Group K-Fold with 5 random splits\n",
    "    np.random.seed(15)  # Ensure reproducibility\n",
    "    k = 5\n",
    "    random_country_splits = np.array_split(np.random.permutation(unique_countries), k)\n",
    "\n",
    "    # Suggesting hyperparameters for XGBoost\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 0.5),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 2, 15),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),\n",
    "        'random_state': 22,\n",
    "        'enable_categorical': True,\n",
    "        'eval_metric': 'mae',\n",
    "        'early_stopping_rounds': 10,\n",
    "        'tree_method': 'hist'\n",
    "    }\n",
    "\n",
    "    # Suggesting the percentage of features to keep\n",
    "    feature_space_percentage = trial.suggest_float('feature_percentage', 0.025, 0.125)  # 2.5% to 12.5%\n",
    "\n",
    "    # Calculates the number of features to keep\n",
    "    num_top_features = int(len(feature_importance) * feature_space_percentage)\n",
    "\n",
    "    mandatory_indices = [i for i, col in enumerate(X_train.columns) if col in mandatory_features]\n",
    "\n",
    "    non_mandatory_indices = [\n",
    "        i for i in range(len(feature_importance)) if i not in mandatory_indices\n",
    "    ]\n",
    "    sorted_non_mandatory_indices = [non_mandatory_indices[i] for i in np.argsort(feature_importance[non_mandatory_indices])[::-1]]\n",
    "    top_disease_feature_indices = sorted_non_mandatory_indices[:num_top_features]\n",
    "\n",
    "    # Combine mandatory and selected non-mandatory features\n",
    "    final_feature_indices = set(top_disease_feature_indices).union(mandatory_indices)\n",
    "    top_features = [X_train.columns[i] for i in final_feature_indices]\n",
    "\n",
    "    X_train_selected = X_train[top_features]\n",
    "\n",
    "    mae_scores = []\n",
    "\n",
    "    for validation_split in random_country_splits:\n",
    "        # Create train and validation splits based on countries\n",
    "        is_valid = pd.Series(countries).isin(validation_split).values\n",
    "        is_train = ~is_valid\n",
    "\n",
    "        X_tr, X_val = X_train_selected.iloc[is_train], X_train_selected.iloc[is_valid]\n",
    "        y_tr, y_val = y_train.iloc[is_train], y_train.iloc[is_valid]\n",
    "\n",
    "        # Train the model\n",
    "        model = XGBRegressor(**params)\n",
    "        model.fit(\n",
    "            X_tr,\n",
    "            y_tr,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        preds = model.predict(X_val)\n",
    "        mae_scores.append(mean_absolute_error(y_val, preds))\n",
    "\n",
    "    # Return average MAE across splits\n",
    "    return sum(mae_scores) / len(mae_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79f8682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature_importance_group_cv(X_train, y_train, countries, model, kf_splits=10):\n",
    "    # Unique countries and random splits for group CV\n",
    "    unique_countries = pd.Series(countries).unique()\n",
    "    np.random.seed(42)\n",
    "    random_country_splits = np.array_split(np.random.permutation(unique_countries), kf_splits)\n",
    "    \n",
    "    feature_importance = np.zeros(len(X_train.columns))\n",
    "    \n",
    "    for validation_split in random_country_splits:\n",
    "        # Create train and validation splits based on countries\n",
    "        is_valid = pd.Series(countries).isin(validation_split).values\n",
    "        is_train = ~is_valid\n",
    "\n",
    "        X_tr, X_val = X_train.iloc[is_train], X_train.iloc[is_valid]\n",
    "        y_tr, y_val = y_train.iloc[is_train], y_train.iloc[is_valid]\n",
    "\n",
    "        # Fit the model and get feature importances\n",
    "        model.fit(X_tr, y_tr,)\n",
    "        feature_importance += model.feature_importances_ / kf_splits\n",
    "\n",
    "    return feature_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c487898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_impute_original.iloc[:,:-11].columns)-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ae6de29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ISO3', 'Year',\n",
       "       'African swine fever virus (Inf. with)_cattle_incidence',\n",
       "       'Anthrax_cattle_incidence',\n",
       "       'Aujeszky's disease virus (Inf. with)_cattle_incidence',\n",
       "       'Avian chlamydiosis_cattle_incidence',\n",
       "       'Bluetongue virus (Inf. with)_cattle_incidence',\n",
       "       'Bovine anaplasmosis_cattle_incidence',\n",
       "       'Bovine babesiosis_cattle_incidence',\n",
       "       'Bovine cysticercosis (-2014)_cattle_incidence',\n",
       "       ...\n",
       "       'Pigeon rotavirus_poultry_vaccine_cov',\n",
       "       'Pullorum disease_poultry_vaccine_cov',\n",
       "       'Rabies virus (Inf. with)_poultry_vaccine_cov',\n",
       "       'Reovirus (inf with)_poultry_vaccine_cov',\n",
       "       'Salmonellosis (S. abortusovis)_poultry_vaccine_cov',\n",
       "       'Sheep pox and goat pox_poultry_vaccine_cov',\n",
       "       'Trichomonosis_poultry_vaccine_cov',\n",
       "       'Trypanosomosis (tsetse-transmitted) (-2021)_poultry_vaccine_cov',\n",
       "       'Turkey rhinotracheitis (2006-)_poultry_vaccine_cov',\n",
       "       'West Nile Fever_poultry_vaccine_cov'],\n",
       "      dtype='object', length=303)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_impute_original.iloc[:,:-11].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "14856e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISO3</th>\n",
       "      <th>Year</th>\n",
       "      <th>Anthrax_swine_incidence</th>\n",
       "      <th>Bovine tuberculosis (-2018)_cattle_vaccine_cov</th>\n",
       "      <th>Taenia solium (Inf. with) (Porcine cysticercosis)_swine_incidence</th>\n",
       "      <th>New world screwworm (Cochliomyia hominivorax)_cattle_incidence</th>\n",
       "      <th>Influenza A virus (Inf. with)_swine_vaccine_cov</th>\n",
       "      <th>Salmonellosis (S. abortusovis)_cattle_incidence</th>\n",
       "      <th>Rabies virus (Inf. with)_swine_incidence</th>\n",
       "      <th>Swine vesicular disease (-2014)_swine_incidence</th>\n",
       "      <th>...</th>\n",
       "      <th>prop_milk_cows</th>\n",
       "      <th>prop_laying_chickens</th>\n",
       "      <th>total pigs</th>\n",
       "      <th>total poultry</th>\n",
       "      <th>total cattle</th>\n",
       "      <th>area_sq_km</th>\n",
       "      <th>pig_density</th>\n",
       "      <th>poultry_density</th>\n",
       "      <th>cattle_density</th>\n",
       "      <th>Region_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFG</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.698362</td>\n",
       "      <td>0.846399</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14414000.0</td>\n",
       "      <td>3723000.0</td>\n",
       "      <td>930332.238549</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.493390</td>\n",
       "      <td>4.001796</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFG</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.681265</td>\n",
       "      <td>0.845588</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10880000.0</td>\n",
       "      <td>4110000.0</td>\n",
       "      <td>930332.238549</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.694747</td>\n",
       "      <td>4.417777</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFG</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.688547</td>\n",
       "      <td>0.907582</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9035000.0</td>\n",
       "      <td>4357000.0</td>\n",
       "      <td>930332.238549</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.711584</td>\n",
       "      <td>4.683273</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFG</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.695469</td>\n",
       "      <td>0.851343</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10689000.0</td>\n",
       "      <td>4745000.0</td>\n",
       "      <td>930332.238549</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.489444</td>\n",
       "      <td>5.100328</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AFG</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.699004</td>\n",
       "      <td>0.853527</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10193000.0</td>\n",
       "      <td>4721000.0</td>\n",
       "      <td>930332.238549</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.956301</td>\n",
       "      <td>5.074531</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 314 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ISO3    Year  Anthrax_swine_incidence  \\\n",
       "0  AFG  2005.0                      NaN   \n",
       "1  AFG  2006.0                      NaN   \n",
       "2  AFG  2007.0                      NaN   \n",
       "3  AFG  2008.0                      NaN   \n",
       "4  AFG  2009.0                      NaN   \n",
       "\n",
       "   Bovine tuberculosis (-2018)_cattle_vaccine_cov  \\\n",
       "0                                             NaN   \n",
       "1                                             NaN   \n",
       "2                                             NaN   \n",
       "3                                             NaN   \n",
       "4                                             NaN   \n",
       "\n",
       "   Taenia solium (Inf. with) (Porcine cysticercosis)_swine_incidence  \\\n",
       "0                                                NaN                   \n",
       "1                                                NaN                   \n",
       "2                                                NaN                   \n",
       "3                                                NaN                   \n",
       "4                                                NaN                   \n",
       "\n",
       "   New world screwworm (Cochliomyia hominivorax)_cattle_incidence  \\\n",
       "0                                                NaN                \n",
       "1                                                NaN                \n",
       "2                                                NaN                \n",
       "3                                                NaN                \n",
       "4                                                NaN                \n",
       "\n",
       "   Influenza A virus (Inf. with)_swine_vaccine_cov  \\\n",
       "0                                              NaN   \n",
       "1                                              NaN   \n",
       "2                                              NaN   \n",
       "3                                              NaN   \n",
       "4                                              NaN   \n",
       "\n",
       "   Salmonellosis (S. abortusovis)_cattle_incidence  \\\n",
       "0                                              NaN   \n",
       "1                                              NaN   \n",
       "2                                              NaN   \n",
       "3                                              NaN   \n",
       "4                                              NaN   \n",
       "\n",
       "   Rabies virus (Inf. with)_swine_incidence  \\\n",
       "0                                       NaN   \n",
       "1                                       NaN   \n",
       "2                                       NaN   \n",
       "3                                       NaN   \n",
       "4                                       NaN   \n",
       "\n",
       "   Swine vesicular disease (-2014)_swine_incidence  ...  prop_milk_cows  \\\n",
       "0                                              NaN  ...        0.698362   \n",
       "1                                              NaN  ...        0.681265   \n",
       "2                                              NaN  ...        0.688547   \n",
       "3                                              NaN  ...        0.695469   \n",
       "4                                              NaN  ...        0.699004   \n",
       "\n",
       "   prop_laying_chickens  total pigs  total poultry  total cattle  \\\n",
       "0              0.846399         NaN     14414000.0     3723000.0   \n",
       "1              0.845588         NaN     10880000.0     4110000.0   \n",
       "2              0.907582         NaN      9035000.0     4357000.0   \n",
       "3              0.851343         NaN     10689000.0     4745000.0   \n",
       "4              0.853527         NaN     10193000.0     4721000.0   \n",
       "\n",
       "      area_sq_km  pig_density  poultry_density  cattle_density  Region_encoded  \n",
       "0  930332.238549          NaN        15.493390        4.001796              12  \n",
       "1  930332.238549          NaN        11.694747        4.417777              12  \n",
       "2  930332.238549          NaN         9.711584        4.683273              12  \n",
       "3  930332.238549          NaN        11.489444        5.100328              12  \n",
       "4  930332.238549          NaN        10.956301        5.074531              12  \n",
       "\n",
       "[5 rows x 314 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shuffling column order so that iterative imputation equally benefits imputations of disease incidence and vaccination coverage\n",
    "# Identify the columns\n",
    "first_two_columns = df_impute_original.columns[:2]\n",
    "last_11_columns = df_impute_original.columns[-11:]\n",
    "middle_columns = df_impute_original.columns[2:-11]\n",
    "\n",
    "# Shuffle the middle columns\n",
    "shuffled_middle_columns = random.sample(list(middle_columns), len(middle_columns))\n",
    "\n",
    "# Combine the columns\n",
    "new_column_order = list(first_two_columns) + shuffled_middle_columns + list(last_11_columns)\n",
    "\n",
    "# Reorder the DataFrame\n",
    "df_impute_original = df_impute_original[new_column_order]\n",
    "\n",
    "df_impute_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4c74a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_gamma_vectorized(lower, upper, mean, n_samples, rng):\n",
    "    # Initialize the output array with NaN as default\n",
    "    samples = np.full((len(mean) * n_samples), np.nan)\n",
    "\n",
    "    # Handle cases where mean == 0\n",
    "    mean = np.where(mean == 0, 0.001, mean)\n",
    "\n",
    "    # Swap bounds where lower > upper\n",
    "    invalid_bounds = lower > upper\n",
    "    if invalid_bounds.any():\n",
    "        lower[invalid_bounds], upper[invalid_bounds] = upper[invalid_bounds], lower[invalid_bounds]\n",
    "\n",
    "    # Identify rows where both lower and upper are 0\n",
    "    zero_bounds = (lower == 0) & (upper == 0)\n",
    "    if zero_bounds.any():\n",
    "        zero_indices = np.where(zero_bounds)[0]\n",
    "        for zero_row in zero_indices:\n",
    "            samples[zero_row * n_samples:(zero_row + 1) * n_samples] = 0\n",
    "\n",
    "    #Identify valid rows where lower, upper, and mean are not NaN\n",
    "    valid_rows = ~np.isnan(mean) & ~np.isnan(lower) & ~np.isnan(upper) & ~zero_bounds\n",
    "\n",
    "    if valid_rows.any():\n",
    "        # Calculate variance, shape, and scale for valid rows\n",
    "        variance = ((upper[valid_rows] - lower[valid_rows]) / 4) ** 2\n",
    "        variance = np.where(variance == 0, 1e-6, variance)  # Handle zero variance\n",
    "\n",
    "        shape = mean[valid_rows] ** 2 / variance\n",
    "        scale = variance / mean[valid_rows]\n",
    "\n",
    "        # Separate valid and invalid gamma parameter rows\n",
    "        valid_params = (shape > 0) & (scale > 0)\n",
    "\n",
    "        #Handle rows with valid gamma parameters\n",
    "        valid_indices = np.where(valid_rows)[0][valid_params]\n",
    "        if valid_params.any():\n",
    "            for i, valid_row in enumerate(valid_indices):\n",
    "                sampled_values = rng.gamma(shape[valid_params][i], scale[valid_params][i], n_samples)\n",
    "                samples[valid_row * n_samples:(valid_row + 1) * n_samples] = sampled_values\n",
    "\n",
    "        # Handle rows with invalid gamma parameters\n",
    "        invalid_params = ~valid_params\n",
    "        invalid_indices = np.where(valid_rows)[0][invalid_params]\n",
    "        for invalid_row in invalid_indices:\n",
    "            if lower[invalid_row] <= upper[invalid_row]:  # Ensure valid bounds\n",
    "                sampled_values = rng.uniform(lower[invalid_row], upper[invalid_row], n_samples)\n",
    "                samples[invalid_row * n_samples:(invalid_row + 1) * n_samples] = sampled_values\n",
    "\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8844c629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_beta_vectorized(lower, upper, mean, n_samples, rng):\n",
    "    # Initialize the output array with NaN as default\n",
    "    samples = np.full((len(mean) * n_samples), np.nan)\n",
    "\n",
    "    # Handle cases where mean == 0\n",
    "    mean = np.where(mean == 0, 0.001, mean)\n",
    "\n",
    "    # Swap bounds where lower > upper\n",
    "    invalid_bounds = lower > upper\n",
    "    if invalid_bounds.any():\n",
    "        lower[invalid_bounds], upper[invalid_bounds] = upper[invalid_bounds], lower[invalid_bounds]\n",
    "\n",
    "    # Identify valid rows where lower, upper, and mean are not NaN\n",
    "    valid_rows = ~np.isnan(mean) & ~np.isnan(lower) & ~np.isnan(upper)\n",
    "\n",
    "    if valid_rows.any():\n",
    "        # Calculate variance, alpha, and beta for valid rows\n",
    "        variance = ((upper[valid_rows] - lower[valid_rows]) / 4) ** 2\n",
    "        variance = np.where(variance == 0, 1e-6, variance)  # Handle zero variance\n",
    "\n",
    "        alpha = ((mean[valid_rows] ** 2) * (1 - mean[valid_rows]) / variance) - mean[valid_rows]\n",
    "        beta = alpha * (1 / mean[valid_rows] - 1)\n",
    "\n",
    "        # Separate valid and invalid beta parameter rows\n",
    "        valid_params = (alpha > 0) & (beta > 0)\n",
    "\n",
    "        # Handle rows with valid beta parameters\n",
    "        valid_indices = np.where(valid_rows)[0][valid_params]\n",
    "        if valid_params.any():\n",
    "            for i, valid_row in enumerate(valid_indices):\n",
    "                sampled_values = rng.beta(alpha[valid_params][i], beta[valid_params][i], n_samples)\n",
    "                samples[valid_row * n_samples:(valid_row + 1) * n_samples] = sampled_values\n",
    "\n",
    "        #Handle rows with invalid beta parameters\n",
    "        invalid_params = ~valid_params\n",
    "        invalid_indices = np.where(valid_rows)[0][invalid_params]\n",
    "        for invalid_row in invalid_indices:\n",
    "            if lower[invalid_row] <= upper[invalid_row]:  # Ensure valid bounds\n",
    "                sampled_values = rng.uniform(lower[invalid_row], upper[invalid_row], n_samples)\n",
    "                samples[invalid_row * n_samples:(invalid_row + 1) * n_samples] = sampled_values\n",
    "\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b387bb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is done to remove outliers from influencing imputations \n",
    "def update_flagged_diseases(df, diseases_flagged):\n",
    "    \"\"\"\n",
    "    Set flagged disease columns to NaN for rows with matching ISO3 and Year pairs.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The dataframe to update.\n",
    "        diseases_flagged (dict): A dictionary where keys are column names (diseases) \n",
    "                                 and values are sets of (ISO3, Year) pairs.\n",
    "\n",
    "    Returns:\n",
    "        None (modifies df in place)\n",
    "    \"\"\"\n",
    "    for disease_column, flagged_pairs in diseases_flagged.items():\n",
    "        if flagged_pairs:  \n",
    "            # Create a mask for rows matching the flagged ISO3-Year pairs\n",
    "            mask = df.apply(lambda row: (row['ISO3'], row['Year']) in flagged_pairs, axis=1)\n",
    "            print(f'Identified outliers for disease: {disease_column}')\n",
    "            print(f\"Flagged and removed pairs (extreme outliers): {flagged_pairs}\")\n",
    "\n",
    "            # Set the disease column values to NaN for the flagged rows\n",
    "            df.loc[mask, disease_column] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1459db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "bold_start = '\\033[1m'\n",
    "bold_end = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03ad8b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_samples = 50\n",
    "n_bootstrap = 100\n",
    "bayesian_trials=1000 #Iterations of bayesian tuning\n",
    "\n",
    "seed = 2\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "\n",
    "\n",
    "# List of columns to process\n",
    "incidence_cols = [col for col in df_impute_sampling.columns if ('_incidence' in col) & ('upper' not in col) & ('lower' not in col)]\n",
    "vaccine_cols = [col for col in df_impute_sampling.columns if ('_vaccine' in col) & ('upper' not in col) & ('lower' not in col)]\n",
    "\n",
    "# Reused variables\n",
    "static_columns = ['ISO3', 'Year', 'GDP per Capita', 'prop_milk_cows', 'prop_laying_chickens', \n",
    "                  'Region_encoded', 'total poultry', 'total cattle', 'total pigs', \n",
    "                  'area_sq_km', 'cattle_density', 'poultry_density', 'pig_density']\n",
    "\n",
    "# Repeat rows of static columns to match the number of samples\n",
    "static_data = df_impute_sampling[static_columns].loc[\n",
    "    np.repeat(df_impute_sampling.index, n_samples)\n",
    "].reset_index(drop=True)\n",
    "\n",
    "synthetic_data_dict={}\n",
    "\n",
    "for col in static_columns:\n",
    "    synthetic_data_dict[col] = static_data[col].values\n",
    "\n",
    "synthetic_df = pd.DataFrame(synthetic_data_dict)\n",
    "\n",
    "\n",
    "sampled_columns = {}\n",
    "\n",
    "#These are samples from the CIs of estimates\n",
    "for col in incidence_cols:\n",
    "    lower = df_impute_sampling[col.replace('_incidence', '_incidence_lower')].values\n",
    "    upper = df_impute_sampling[col.replace('_incidence', '_incidence_upper')].values\n",
    "    mean = df_impute_sampling[col].values\n",
    "\n",
    "    sampled_values = sample_from_gamma_vectorized(lower, upper, mean, n_samples, rng)\n",
    "    \n",
    "    sampled_columns[col] = sampled_values\n",
    "\n",
    "for col in vaccine_cols:\n",
    "    lower = df_impute_sampling[col.replace('_vaccine_cov', '_vaccine_cov_lower')].values\n",
    "    upper = df_impute_sampling[col.replace('_vaccine_cov', '_vaccine_cov_upper')].values\n",
    "    mean = df_impute_sampling[col].values\n",
    "\n",
    "    sampled_values = sample_from_beta_vectorized(lower, upper, mean, n_samples, rng)\n",
    "    \n",
    "    sampled_columns[col] = sampled_values\n",
    "\n",
    "sampled_df = pd.DataFrame(sampled_columns)\n",
    "synthetic_df = pd.concat([synthetic_df, sampled_df], axis=1)\n",
    "\n",
    "\n",
    "synthetic_df = synthetic_df[df_impute_original.columns]\n",
    "\n",
    "# Original dataset with missing values\n",
    "data_iterative_random = df_impute_original.copy()\n",
    "\n",
    "# Valid diseases per animal ('qualifying variables' that can be selected for ML imputation)\n",
    "valid_diseases={}\n",
    "for col in data_iterative_random.columns:\n",
    "    \n",
    "    #Ensuring good quality with a threshold (30+ countries must have data)\n",
    "    if (data_iterative_random[col].notna().sum() >= 30):\n",
    "\n",
    "        if ('_incidence' in col):\n",
    "            parts = col.split('_')\n",
    "            disease = '_'.join(parts[:-2])  \n",
    "            animal_col = parts[-2]         #\n",
    "            \n",
    "    \n",
    "            if animal_col not in valid_diseases:\n",
    "                valid_diseases[animal_col] = []\n",
    "            if disease not in valid_diseases[animal_col]:\n",
    "                valid_diseases[animal_col].append(disease)\n",
    "        \n",
    "        if ('_vaccine_cov' in col):    \n",
    "            parts = col.split('_')\n",
    "            disease = '_'.join(parts[:-3]) \n",
    "            animal_col = parts[-3]         \n",
    "    \n",
    "            if animal_col not in valid_diseases:\n",
    "                valid_diseases[animal_col] = []\n",
    "            if disease not in valid_diseases[animal_col]:\n",
    "                valid_diseases[animal_col].append(disease)\n",
    "\n",
    "\n",
    "\n",
    "# A DataFrame to store the best hyperparameters for each column\n",
    "hyperparams_table_iterative_random = pd.DataFrame()\n",
    "\n",
    "\n",
    "# lists to store metrics\n",
    "original_dataset_mae_scores = []\n",
    "original_dataset_residuals_std_scores = []\n",
    "expanded_dataset_mae_scores = []\n",
    "expanded_dataset_residuals_std_scores = []\n",
    "\n",
    "prediction_intervals_lower_bootstrap = []  # variability via bootstrpaping\n",
    "prediction_intervals_upper_bootstrap = []  # \n",
    "prediction_intervals_lower_residuals = []  # variability via residuals\n",
    "prediction_intervals_upper_residuals = []  # \n",
    "prediction_intervals_lower_asymmetric = []  # total variability\n",
    "prediction_intervals_upper_asymmetric = []  #\n",
    "\n",
    "diseases_flagged = {}\n",
    "\n",
    "\n",
    "\n",
    "for column in data_iterative_random.columns:\n",
    "    if ((\"_swine\" in column) or (\"_cattle\" in column) or (\"_poultry\" in column)) and ('_incidence' in column):\n",
    "       \n",
    "        # Assign the appropriate population column\n",
    "        if '_cattle' in column:\n",
    "            pop_col = 'total cattle'\n",
    "        elif '_poultry' in column:\n",
    "            pop_col = 'total poultry'\n",
    "        elif '_swine' in column:\n",
    "            pop_col = 'total pigs'\n",
    "            \n",
    "        \n",
    "        # Identify outliers and store them in the dictionary\n",
    "        \n",
    "        outlier_data= identify_outliers(data_iterative_random, column, pop_col)\n",
    "        if len(outlier_data)>0:\n",
    "            diseases_flagged[column]=outlier_data\n",
    "            \n",
    "\n",
    "\n",
    "#remove outliers\n",
    "update_flagged_diseases(data_iterative_random, diseases_flagged)\n",
    "update_flagged_diseases(synthetic_df, diseases_flagged) \n",
    "\n",
    "update_flagged_diseases(df_impute_sampling, diseases_flagged)\n",
    "\n",
    "\n",
    "col_num = 0\n",
    "\n",
    "tot_columns=0\n",
    "\n",
    "for column in data_iterative_random.columns:\n",
    "    if df_impute_original[df_impute_original[column].notna()]['ISO3'].nunique() >= 30:\n",
    "        tot_columns+=1\n",
    "\n",
    "for column in data_iterative_random.columns:\n",
    "    \n",
    "    if '_cattle' in column:\n",
    "        weights = total_cattle_weights  # Use predicted values as weights (example)\n",
    "        col_to_add = 'total cattle'\n",
    "        col_to_add2='cattle_density'\n",
    "        animal='cattle'\n",
    "    elif '_poultry' in column:\n",
    "        weights = total_poultry_weights  # Use predicted values as weights (example)\n",
    "        col_to_add = 'total poultry'\n",
    "        col_to_add2='poultry_density'\n",
    "        animal='poultry'\n",
    "    elif '_swine' in column:\n",
    "        weights = total_swine_weights  # Use predicted values as weights (example)\n",
    "        col_to_add = 'total pigs'\n",
    "        col_to_add2= 'pig_density'\n",
    "        animal='swine'\n",
    "        \n",
    "        \n",
    "    if (\"_swine\" in column) or (\"_cattle\" in column) or (\"_poultry\" in column) :\n",
    "        \n",
    "        \n",
    "        # If at least 30 countries w available data points\n",
    "        if (\n",
    "            (\n",
    "                df_impute_original[df_impute_original[column].notna()]['ISO3'].nunique() >= 30\n",
    "            ) \n",
    "         and df_impute_original[column].isnull().sum() > 0):\n",
    "            col_num += 1\n",
    "            \n",
    "            print(\"Running imputation for \",column,'(',col_num,'/',tot_columns,')')\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            countries_original=data_iterative_random['ISO3']\n",
    "            original_X=data_iterative_random.drop(columns=[column, 'ISO3']).copy()\n",
    "            original_y=data_iterative_random[column].copy()\n",
    "            \n",
    "            countries=synthetic_df['ISO3']\n",
    "            X = synthetic_df.drop(columns=[column, 'ISO3'])  \n",
    "            y = synthetic_df[column]  # The target column\n",
    "            \n",
    "            # Selecting the rows where the target is not missing for training\n",
    "            X_train = X[~y.isnull()]\n",
    "            y_train = y[~y.isnull()]\n",
    "            countries = countries[~y.isnull()]\n",
    "            \n",
    "            mandatory_features = [\"GDP per Capita\", \"Region_encoded\", \"prop_milk_cows\", \"prop_laying_chickens\", \"Year\",col_to_add,'area_sq_km',col_to_add2]\n",
    "            \n",
    "            model = xgb.XGBRegressor(random_state=20, enable_categorical=True)\n",
    "\n",
    "            \n",
    "            other_animals = set(['cattle','swine','poultry'])-set(animal)\n",
    "\n",
    "            n_samples_threshold = 30 * n_samples\n",
    "            holder = X_train.copy()\n",
    "            \n",
    "            #Only keep pop sizes/densities of animals that can get infected with and spread same disease\n",
    "            for other_animal in other_animals:\n",
    "                #Check if '_vaccine_cov' and '_incidence' columns exist and meet criteria\n",
    "                vaccine_col = column.replace('_incidence', '_vaccine_cov').replace('_' + animal, '_' + other_animal)\n",
    "                incidence_col = column.replace('_vaccine_cov', '_incidence').replace('_' + animal, '_' + other_animal)\n",
    "\n",
    "                vaccine_keep = (\n",
    "                    vaccine_col in X_train.columns and \n",
    "                    X_train[vaccine_col].notna().sum() >= n_samples_threshold\n",
    "                )\n",
    "                incidence_keep = (\n",
    "                    incidence_col in X_train.columns and \n",
    "                    X_train[incidence_col].notna().sum() >= n_samples_threshold\n",
    "                )\n",
    "\n",
    "                if not (vaccine_keep or incidence_keep):\n",
    "                    # Set other_animal population columns to NaN\n",
    "                    holder.loc[:, other_animal + \"_density\"] = np.nan\n",
    "                    holder.loc[:, \"total \" + other_animal] = np.nan\n",
    "\n",
    "            #Filter '_incidence' columns and set irrelevant ones to NaN\n",
    "            #Bulding list of valid diseases\n",
    "            \n",
    "            #Only keep incidence for relevant diseases\n",
    "            valid_diseases_for_animal = set(valid_diseases.get(animal, []))\n",
    "\n",
    "            for col in X_train.columns:\n",
    "                if '_incidence' in col:\n",
    "                    parts = col.split('_')\n",
    "                    disease = '_'.join(parts[:-2])\n",
    "                    animal_col = parts[-2]\n",
    "\n",
    "                    # Check conditions for setting to NaN\n",
    "                    if animal_col != animal and disease not in valid_diseases_for_animal:\n",
    "                        holder[col] = np.nan  # Use a single assignment instead of recreating a full array\n",
    "\n",
    "                                    \n",
    "            feature_importance = compute_feature_importance_group_cv(holder, y_train, countries, model, kf_splits=10)\n",
    "\n",
    "\n",
    "            \n",
    "            # Define and perform the Bayesian hyperparameter optimization for XGBoost\n",
    "            #categorical enabled to account for the region encoding\n",
    "            sampler = optuna.samplers.TPESampler(seed=2, n_startup_trials=100)\n",
    "            study = optuna.create_study(sampler=sampler,direction='minimize', study_name=f\"study_{column}\")\n",
    "            # Optimize the objective function\n",
    "            study.optimize(\n",
    "                lambda trial: objective(\n",
    "                    trial, X_train, y_train, countries, feature_importance, mandatory_features\n",
    "                ),\n",
    "                n_trials=bayesian_trials,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            \n",
    "\n",
    "            # Get the best model and best parameters\n",
    "            best_params = study.best_params\n",
    "            \n",
    "            prop_features=best_params.pop('feature_percentage')# Suggest the percentage of features to keep\n",
    "\n",
    "\n",
    "            print(f'{bold_start}Optimal percent of total features: {prop_features*100:.2f}%{bold_end}')\n",
    "            print(f'{bold_start}Optimal hyperparameters: {best_params}{bold_end}')\n",
    "            \n",
    "\n",
    "            best_model = xgb.XGBRegressor(**best_params,\n",
    "                                          random_state=2,           \n",
    "                                          enable_categorical=True,\n",
    "                                          tree_method='hist'\n",
    "                                         )    # Enable categorical handling\n",
    "            \n",
    "            best_params['feature_percentage']=prop_features\n",
    "            \n",
    "\n",
    "            # Continue with the rest of the feature selection process\n",
    "            num_top_features = int(len(feature_importance) * prop_features)\n",
    "\n",
    "            # Identify indices of mandatory features\n",
    "            mandatory_indices = [i for i, col in enumerate(X_train.columns) if col in mandatory_features]\n",
    "\n",
    "            # Exclude mandatory features from ranking\n",
    "            non_mandatory_indices = [i for i in range(len(feature_importance)) if i not in mandatory_indices]\n",
    "\n",
    "            # Rank non-mandatory features by importance\n",
    "            sorted_non_mandatory_indices = [non_mandatory_indices[i] for i in np.argsort(feature_importance[non_mandatory_indices])[::-1]]\n",
    "\n",
    "            # Select top-ranked non-mandatory features\n",
    "            top_disease_feature_indices = sorted_non_mandatory_indices[:num_top_features]\n",
    "            \n",
    "\n",
    "            # Combine mandatory features with top-ranked non-mandatory features\n",
    "            final_feature_indices = set(top_disease_feature_indices).union(mandatory_indices)\n",
    "\n",
    "            # Map indices back to feature names\n",
    "            top_features = [X_train.columns[i] for i in final_feature_indices]\n",
    "\n",
    "            # Print selected features and their importance\n",
    "            sorted_features = sorted(\n",
    "                [(feature, feature_importance[X_train.columns.get_loc(feature)]) for feature in top_features],\n",
    "                key=lambda x: x[1],\n",
    "                reverse=True\n",
    "            )\n",
    "            \n",
    "            print(\"Selected Features and Importance:\")\n",
    "            for feature, importance in sorted_features:\n",
    "                print(f\"Feature: {feature}, Importance: {importance:.4f}\")\n",
    "\n",
    "            \n",
    "            # Update X_train with selected features\n",
    "            X_train = X_train[top_features]\n",
    "            original_X = original_X[top_features]\n",
    "            X = X[top_features]\n",
    "            \n",
    "            \n",
    "            best_model.fit(X_train, y_train) #now train on full dataset\n",
    "            \n",
    "            \n",
    "            # Use the best model to predict missing values\n",
    "            X_missing_original = original_X[original_y.isnull()]\n",
    "            X_missing=X[y.isnull()]\n",
    "            \n",
    "            \n",
    "            # Final prediction from the best model\n",
    "            y_pred = best_model.predict(X_missing_original)\n",
    "            y_pred = clip_values(y_pred,column)  # Ensure predictions stay between 0 and 1\n",
    "            \n",
    "            # Compute residuals and MAE based on the original dataset\n",
    "            y_train_original = original_y[~original_y.isnull()]\n",
    "            X_train_original = original_X[~original_y.isnull()]\n",
    "            \n",
    "            # Predict on the original dataset using the trained model\n",
    "            y_train_pred_original = best_model.predict(X_train_original)\n",
    "            \n",
    "            # Calculate residuals and MAE for the original dataset\n",
    "            residuals_original = y_train_original - y_train_pred_original\n",
    "            mae_original = np.mean(np.abs(residuals_original))\n",
    "            \n",
    "                           \n",
    "            #Print residual statistics\n",
    "            print(f\"Residuals for {column} - original dataset Mean: {residuals_original.mean()}, Std Dev: {residuals_original.std()}\")\n",
    "            \n",
    "            \n",
    "            #ADDED HERE: Compute residual variability for this column\n",
    "            y_train_pred = best_model.predict(X_train)\n",
    "            residuals = y_train - y_train_pred\n",
    "            sigma_residual = residuals.std()\n",
    "            \n",
    "            # Asymmetric residuals\n",
    "            sigma_residual_neg = residuals[residuals < 0].std()  # Made change: Capture under-predictions\n",
    "            sigma_residual_pos = residuals[residuals > 0].std()  # Made change: Capture over-predictions\n",
    "            \n",
    "            \n",
    "            # Generate predictions for the expanded dataset\n",
    "            expanded_predictions = best_model.predict(X_train)  # Predict using the full expanded dataset\n",
    "            \n",
    "            # Calculate residuals only for rows with known values in the original dataset\n",
    "            expanded_y = synthetic_df[column][~synthetic_df[column].isnull()]\n",
    "            expanded_residuals = expanded_y[~synthetic_df[column].isnull()]-expanded_predictions\n",
    "            # Calculate MAE for the expanded dataset\n",
    "            expanded_mae = np.mean(np.abs(expanded_residuals))\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Bootstrap to compute prediction intervals for the imputed values\n",
    "            bootstrap_predictions = []\n",
    "            \n",
    "            for i in range(n_bootstrap):\n",
    "                # Resample indices with replacement, using 100% of the data\n",
    "                resampled_indices = resample(X_train.index, n_samples=int(1.0 * len(X_train)), replace=True)\n",
    "                \n",
    "                # Use these indices to select rows from X_train and y_train\n",
    "                X_resampled = X_train.loc[resampled_indices]\n",
    "                y_resampled = y_train.loc[resampled_indices]\n",
    "                \n",
    "                # Fit the model on the resampled data\n",
    "                best_model.fit(X_resampled, y_resampled)\n",
    "                \n",
    "                # Predict on the missing values using the model trained on the resampled data\n",
    "                y_pred_bootstrap = best_model.predict(X_missing_original)\n",
    "                \n",
    "                # Store the predictions from the current bootstrap iteration\n",
    "                bootstrap_predictions.append(y_pred_bootstrap)\n",
    "            \n",
    "            # Convert to numpy array for easier computation\n",
    "            bootstrap_predictions = np.array(bootstrap_predictions)\n",
    "            \n",
    "            #Compute variability in bootstrap predictions relative to the original model\n",
    "            bootstrap_median = np.percentile(bootstrap_predictions, 50, axis=0)\n",
    "            bootstrap_width_lower = bootstrap_median - np.percentile(bootstrap_predictions, 2.5, axis=0)\n",
    "            bootstrap_width_upper = np.percentile(bootstrap_predictions, 97.5, axis=0) - bootstrap_median\n",
    "            \n",
    "            \n",
    "            \n",
    "            lower_bound_residuals=y_pred - 1.96 * sigma_residual_neg\n",
    "            upper_bound_residuals= y_pred + 1.96 * sigma_residual_pos\n",
    "            lower_bound_bootstrap=y_pred - bootstrap_width_lower\n",
    "            upper_bound_bootstrap=y_pred + bootstrap_width_upper\n",
    "\n",
    "\n",
    "            # Asymmetric Prediction Intervals\n",
    "            lower_bound_asymmetric = y_pred - bootstrap_width_lower - 1.96 * sigma_residual_neg\n",
    "            upper_bound_asymmetric = y_pred + bootstrap_width_upper + 1.96 * sigma_residual_pos\n",
    "            \n",
    "            print('lower_bound_asymmetric widths',np.abs(-bootstrap_width_lower - 1.96 * sigma_residual_neg))\n",
    "            print('lower_bound_asymmetric widths',bootstrap_width_upper + 1.96 * sigma_residual_pos)\n",
    "\n",
    "            \n",
    "\n",
    "                \n",
    "                \n",
    "            mae_expanded_best=expanded_mae\n",
    "            mae_original_best=mae_original\n",
    "            residuals_std_best=residuals_original.std()\n",
    "            expanded_dataset_residuals_std_scores_best=residuals.std()\n",
    "            \n",
    "            synthetic_df_col_best=synthetic_df[column]\n",
    "            y_pred_best=y_pred\n",
    "            synthetic_pred_best=clip_values(best_model.predict(X_missing),column)\n",
    "            lower_residuals_best=lower_bound_residuals\n",
    "            upper_residuals_best=upper_bound_residuals\n",
    "            lower_bootstrap_best=lower_bound_bootstrap\n",
    "            upper_bootstrap_best=upper_bound_bootstrap\n",
    "            PI_low_asym_best=lower_bound_asymmetric\n",
    "            PI_up_asym_best=upper_bound_asymmetric\n",
    "            kept_best_params=best_params\n",
    "\n",
    "\n",
    "            data_iterative_random.loc[original_y[original_y.isnull()].index, column] = y_pred_best\n",
    "            \n",
    "            #Fill in the synthetic_df column before predictions with the best sampled values \n",
    "            synthetic_df[column]=synthetic_df_col_best\n",
    "\n",
    "            #Fill in synthetic df as well so that model iteratively \"predicts better\"\n",
    "            synthetic_df.loc[y[y.isnull()].index, column] = synthetic_pred_best\n",
    "\n",
    "            original_dataset_mae_scores.append(mae_original_best)\n",
    "            original_dataset_residuals_std_scores.append(residuals_std_best)     \n",
    "            #Save residual and MAE metrics for the expanded dataset\n",
    "            expanded_dataset_mae_scores.append(mae_expanded_best)\n",
    "            expanded_dataset_residuals_std_scores.append(expanded_dataset_residuals_std_scores_best)\n",
    "\n",
    "            prediction_intervals_lower_bootstrap.append(lower_bootstrap_best) # variability via bootstrpaping\n",
    "            prediction_intervals_upper_bootstrap.append(upper_bootstrap_best)  # \n",
    "            prediction_intervals_lower_residuals.append(lower_residuals_best)  # variability via residuals\n",
    "            prediction_intervals_upper_residuals.append(upper_residuals_best)  # \n",
    "            prediction_intervals_lower_asymmetric.append(PI_low_asym_best)\n",
    "            prediction_intervals_upper_asymmetric.append(PI_up_asym_best)\n",
    "            # Store the best parameters in the hyperparams_table_iterative_random\n",
    "            hyperparams_table_iterative_random[column] = pd.Series(kept_best_params)\n",
    "                        \n",
    "                        \n",
    "# Transposing the hyperparams_table_iterative_random so that rows represent columns and columns represent hyperparameters\n",
    "hyperparams_table_iterative_random = hyperparams_table_iterative_random.T\n",
    "\n",
    "hyperparams_table_iterative_random['mean_absolute_error'] = expanded_dataset_mae_scores\n",
    "hyperparams_table_iterative_random['residuals_std'] = expanded_dataset_residuals_std_scores\n",
    "\n",
    "print(\"\\nHyperparameter table:\")\n",
    "print(hyperparams_table_iterative_random)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b02c1810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction_intervals_lower_bootstrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "76e5fef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_iso3s_dropdata_iterative_random = data_iterative_random.replace(-0.0, 0.0)\n",
    "hyperparams_table_iterative_random = hyperparams_table_iterative_random.replace(-0.0, 0.0)\n",
    "\n",
    "# Split into \"_incidence\"\n",
    "df_incidence_hyperparams = hyperparams_table_iterative_random[\n",
    "    hyperparams_table_iterative_random.index.str.endswith('_incidence')\n",
    "].copy()\n",
    "\n",
    "df_incidence_hyperparams.index=[i.replace('_swine','_pigs') for i in df_incidence_hyperparams.index]\n",
    "df_incidence_hyperparams.rename(columns={'mean_absolute_error':'mean_absolute_error (Cases per 100,000 per Year)',\n",
    "                                         'residuals_std':'std_residuals (Cases per 100,000 per Year)'},inplace=True)\n",
    "\n",
    "# Split into \"_vaccine_cov\"\n",
    "df_vaccine_cov_hyperparams = hyperparams_table_iterative_random[\n",
    "    hyperparams_table_iterative_random.index.str.endswith('_vaccine_cov')\n",
    "].copy()\n",
    "\n",
    "df_vaccine_cov_hyperparams.index=[i.replace('_swine','_pigs') for i in df_vaccine_cov_hyperparams.index]\n",
    "\n",
    "df_vaccine_cov_hyperparams['mean_absolute_error (Vaccination Coverage %)']=df_vaccine_cov_hyperparams['mean_absolute_error']*100\n",
    "df_vaccine_cov_hyperparams['std_residuals (Vaccination Coverage %)']=df_vaccine_cov_hyperparams['residuals_std']*100\n",
    "df_vaccine_cov_hyperparams.drop(columns=['mean_absolute_error','residuals_std'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4fa49570",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incidence_hyperparams.to_csv(os.path.join(notebook_dir,'Disease Incidence','Supplementary- Disease Incidence XGBoost Model Performance, Selected Hyperparameters.csv'),index=True, encoding='utf-8-sig')\n",
    "\n",
    "df_vaccine_cov_hyperparams.to_csv(os.path.join(notebook_dir,'Vaccination Coverage','Supplementary- Vaccination Coverage XGBoost Model Performance, Selected Hyperparameters.csv'),index=True, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aa12fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterative_random_incidence=data_iterative_random.drop(columns=[i for i in data_iterative_random.columns if '_vaccine_cov' in i])\n",
    "data_iterative_random_vaccine_cov=data_iterative_random.drop(columns=[i for i in data_iterative_random.columns if '_incidence' in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ce32e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterative_random_incidence.to_csv('incidence_all_imputed_XGBoost.csv',index=False,encoding='utf-8-sig')\n",
    "data_iterative_random_vaccine_cov.to_csv('vaccine_cov_all_imputed_XGBoost.csv',index=False,encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a2636bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction_intervals_lower_residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "022ebd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "CI_data = []\n",
    "\n",
    "# Extract relevant columns for swine, cattle, or poultry\n",
    "relevant_columns = [column for column in df_impute_original.columns if ('_swine' in column) or ('_cattle' in column) or ('_poultry' in column)]\n",
    "relevant_columns = [\n",
    "    col for col in relevant_columns \n",
    "    if (df_impute_original[df_impute_original[col].notna()]['ISO3'].nunique() >= 30) \n",
    "]\n",
    "\n",
    "# Loop through prediction intervals and columns\n",
    "for CIs_disease_lower_residuals, CIs_disease_upper_residuals, \\\n",
    "    CIs_disease_lower_bootstrap, CIs_disease_upper_bootstrap, \\\n",
    "    CIs_disease_lower_asymmetric, CIs_disease_upper_asymmetric, \\\n",
    "    column in zip(prediction_intervals_lower_residuals,\n",
    "                  prediction_intervals_upper_residuals,\n",
    "                  prediction_intervals_lower_bootstrap,\n",
    "                  prediction_intervals_upper_bootstrap,\n",
    "                  prediction_intervals_lower_asymmetric,\n",
    "                  prediction_intervals_upper_asymmetric,\n",
    "                  relevant_columns):\n",
    "    \n",
    "    disease, animal = column.split('_', 1)\n",
    "\n",
    "    animal = animal.title()\n",
    "    \n",
    "    # Loop through prediction intervals and corresponding ISO3 and Year\n",
    "    for CI_lower_res, CI_upper_res, CI_lower_boot, CI_upper_boot, CI_lower_asym, CI_upper_asym, ISO3, year in zip(\n",
    "        clip_values(CIs_disease_lower_residuals,column),\n",
    "        clip_values(CIs_disease_upper_residuals,column),\n",
    "        clip_values(CIs_disease_lower_bootstrap,column),\n",
    "        clip_values(CIs_disease_upper_bootstrap,column),\n",
    "        clip_values(CIs_disease_lower_asymmetric,column),\n",
    "        clip_values(CIs_disease_upper_asymmetric,column),\n",
    "        df_impute_original[df_impute_original[column].isna()]['ISO3'],\n",
    "        df_impute_original[df_impute_original[column].isna()]['Year']):\n",
    "\n",
    "        CI_data.append([\n",
    "            ISO3, year, animal, disease, \n",
    "            CI_lower_res, CI_upper_res,\n",
    "            CI_lower_boot, CI_upper_boot,\n",
    "            CI_lower_asym, CI_upper_asym\n",
    "        ])\n",
    "\n",
    "CI_df = pd.DataFrame(CI_data, columns=[\n",
    "    'ISO3', 'Year', 'Animal', 'Disease', \n",
    "    'Incidence Lower (Residual Only)', 'Incidence Upper (Residual Only)',\n",
    "    'Incidence Lower (Bootstrap Only)', 'Incidence Upper (Bootstrap Only)',\n",
    "    'Incidence Lower (Asymmetric)', 'Incidence Upper (Asymmetric)'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f354ebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CI_df_incidence = CI_df[CI_df['Animal'].str.contains('_Incidence')].copy()\n",
    "CI_df_incidence['Animal'] = CI_df_incidence['Animal'].str.replace('_Incidence', '', regex=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "54ad7281",
   "metadata": {},
   "outputs": [],
   "source": [
    "CI_df_vacc_cov = CI_df[CI_df['Animal'].str.contains('_Vaccine_Cov')].copy()\n",
    "CI_df_vacc_cov['Animal'] = CI_df_vacc_cov['Animal'].str.replace('_Vaccine_Cov', '', regex=False)\n",
    "\n",
    "CI_df_vacc_cov.columns=[i.replace('Incidence','Vaccination Coverage') for i in CI_df_vacc_cov.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "18c136b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CI_df_incidence.to_csv('Incidence_CIs_XGBoost.csv',index=False,encoding='utf-8-sig')\n",
    "CI_df_vacc_cov.to_csv('Vaccine_Coverage_CIs_XGBoost.csv',index=False,encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8c31b7",
   "metadata": {},
   "source": [
    "# Generating dataframe to then append to original estimates (Incidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4a1de535",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_back= df_impute_original.drop(columns=['GDP per Capita',\n",
    " 'prop_milk_cows',\n",
    " 'prop_laying_chickens',\n",
    " 'total pigs',\n",
    " 'total poultry',\n",
    " 'total cattle',\n",
    " 'area_sq_km',\n",
    " 'pig_density',\n",
    " 'poultry_density',\n",
    " 'cattle_density',\n",
    " 'Region_encoded']+[i for i in df_impute_original.columns if '_vaccine_cov' in i])\n",
    "\n",
    "df_melted = pd.melt(df_back, id_vars=['ISO3', 'Year'], var_name='disease_animal', value_name='value')\n",
    "\n",
    "df_melted['Animal'] = df_melted['disease_animal'].apply(lambda x: x.split('_')[-1])\n",
    "df_melted['Disease'] = df_melted['disease_animal'].apply(lambda x: '_'.join(x.split('_')[:-1]))\n",
    "\n",
    "df_melted['Animal']=[i.title() for i in df_melted['Animal']]\n",
    "\n",
    "# Pivot the dataframe to rearrange the diseases into separate columns\n",
    "df_pivot = df_melted.pivot_table(index=['ISO3', 'Year', 'Animal'], columns='Disease', values='value').reset_index()\n",
    "df_pivot.columns.name = None\n",
    "\n",
    "# Melt the dataframe to have 'Disease' as a new column and corresponding 'Incidence' values\n",
    "df_long = pd.melt(df_pivot, id_vars=['ISO3', 'Year', 'Animal'], var_name='Disease', value_name='Incidence')\n",
    "df_long['Animal'] = df_long['Disease'].str.split('_').str[1].str.title()\n",
    "\n",
    "df_long['Disease'] = df_long['Disease'].str.split('_').str[0]\n",
    "\n",
    "df_original = df_long.dropna(subset=['Incidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a5cec5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_df=pd.read_csv('incidence_all_imputed_XGBoost.csv')\n",
    "best_df.columns =[i.replace('_incidence','') for i in best_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "13f820c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed_back=best_df.drop(columns=['GDP per Capita',\n",
    " 'prop_milk_cows',\n",
    " 'prop_laying_chickens',\n",
    " 'total pigs',\n",
    " 'total poultry',\n",
    " 'total cattle',\n",
    " 'area_sq_km',\n",
    " 'pig_density',\n",
    " 'poultry_density',\n",
    " 'cattle_density',\n",
    " 'Region_encoded'])\n",
    "\n",
    "# Melt the dataframe to make 'animal' and disease columns\n",
    "df_melted = pd.melt(df_imputed_back, id_vars=['ISO3', 'Year'], var_name='disease_animal', value_name='value')\n",
    "\n",
    "df_melted['Animal'] = df_melted['disease_animal'].apply(lambda x: x.split('_')[-1])\n",
    "df_melted['Disease'] = df_melted['disease_animal'].apply(lambda x: '_'.join(x.split('_')[:-1]))\n",
    "\n",
    "df_melted['Animal']=[i.title() for i in df_melted['Animal']]\n",
    "\n",
    "# Pivot the dataframe to rearrange the diseases into separate columns\n",
    "df_pivot = df_melted.pivot_table(index=['ISO3', 'Year', 'Animal'], columns='Disease', values='value').reset_index()\n",
    "df_pivot.columns.name = None\n",
    "\n",
    "# Melting the dataframe to have 'Disease' as a new column and corresponding 'Incidence' values\n",
    "df_long = pd.melt(df_pivot, id_vars=['ISO3', 'Year', 'Animal'], var_name='Disease', value_name='Incidence')\n",
    "df_new = df_long.dropna(subset=['Incidence']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5056f489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the \"Incidence\" column from both dataframes for comparison\n",
    "df1_compare = df_new.drop(columns=['Incidence']).reset_index(drop=True)\n",
    "df2_compare = df_original.drop(columns=['Incidence'])\n",
    "\n",
    "# Merge to identify rows that are common between dataframe1 and dataframe2, excluding \"Incidence\"\n",
    "df_diff = pd.merge(df1_compare, df2_compare, how='left', indicator=True)\n",
    "\n",
    "\n",
    "# Boolean indexing to keep rows present only in df_new (i.e., 'left_only')\n",
    "df_filtered = df_new.loc[df_diff['_merge'] == 'left_only'].copy()\n",
    "\n",
    "# Add a new column 'Source' with the value 'Imputed'\n",
    "df_filtered['Source'] = 'Imputed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0761cbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "very_final_df=pd.merge(df_filtered, CI_df_incidence, how='left', indicator=False, on=['ISO3','Year','Animal','Disease'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fa0fdfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "very_final_df.rename(columns={'Incidence':'Incidence (Cases per 100,000)',\n",
    "                             'Incidence Upper': 'Incidence (Cases per 100,000) Upper',\n",
    "                             'Incidence Lower': 'Incidence (Cases per 100,000) Lower'},inplace=True)\n",
    "very_final_df.to_csv(os.path.join(notebook_dir,'Disease Incidence',\"FINAL_ALL_incidence_imputations_to_add.csv\"),index=False,encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4c5a44",
   "metadata": {},
   "source": [
    "# Generating dataframe to then append to original estimates (Vaccination Coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1ef6db01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_back= df_impute_original.drop(columns=['GDP per Capita',\n",
    " 'prop_milk_cows',\n",
    " 'prop_laying_chickens',\n",
    " 'total pigs',\n",
    " 'total poultry',\n",
    " 'total cattle',\n",
    " 'area_sq_km',\n",
    " 'pig_density',\n",
    " 'poultry_density',\n",
    " 'cattle_density',\n",
    " 'Region_encoded']+[i for i in df_impute_original.columns if '_incidence' in i])\n",
    "\n",
    "# Melt the dataframe to make 'animal' and disease columns\n",
    "df_melted = pd.melt(df_back, id_vars=['ISO3', 'Year'], var_name='disease_animal', value_name='value')\n",
    "\n",
    "# Extract 'animal' and 'disease' from 'disease_animal' column\n",
    "df_melted['Animal'] = df_melted['disease_animal'].apply(lambda x: x.split('_')[1])\n",
    "df_melted['Disease'] = df_melted['disease_animal'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "df_melted['Animal']=[i.title() for i in df_melted['Animal']]\n",
    "\n",
    "# Pivot the dataframe to rearrange the diseases into separate columns\n",
    "df_pivot = df_melted.pivot_table(index=['ISO3', 'Year', 'Animal'], columns='Disease', values='value').reset_index()\n",
    "df_pivot.columns.name = None\n",
    "\n",
    "# Melting the dataframe to have 'Disease' as a new column and corresponding 'Incidence' values\n",
    "df_long = pd.melt(df_pivot, id_vars=['ISO3', 'Year', 'Animal'], var_name='Disease', value_name='Vaccination Coverage')\n",
    "df_original = df_long.dropna(subset=['Vaccination Coverage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3aa55408",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_df=pd.read_csv('vaccine_cov_all_imputed_XGBoost.csv')\n",
    "best_df.columns =[i.replace('_vaccine_cov','') for i in best_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e0028b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed_back=best_df.drop(columns=['GDP per Capita',\n",
    " 'prop_milk_cows',\n",
    " 'prop_laying_chickens',\n",
    " 'total pigs',\n",
    " 'total poultry',\n",
    " 'total cattle',\n",
    " 'area_sq_km',\n",
    " 'pig_density',\n",
    " 'poultry_density',\n",
    " 'cattle_density',\n",
    " 'Region_encoded'])\n",
    "\n",
    "# Melt the dataframe to make 'animal' and disease columns\n",
    "df_melted = pd.melt(df_imputed_back, id_vars=['ISO3', 'Year'], var_name='disease_animal', value_name='value')\n",
    "\n",
    "# Extract 'animal' and 'disease' from 'disease_animal' column\n",
    "df_melted['Animal'] = df_melted['disease_animal'].apply(lambda x: x.split('_')[1])\n",
    "df_melted['Disease'] = df_melted['disease_animal'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "df_melted['Animal']=[i.title() for i in df_melted['Animal']]\n",
    "\n",
    "# Pivot the dataframe to rearrange the diseases into separate columns\n",
    "df_pivot = df_melted.pivot_table(index=['ISO3', 'Year', 'Animal'], columns='Disease', values='value').reset_index()\n",
    "df_pivot.columns.name = None\n",
    "\n",
    "# Melt the dataframe to have 'Disease' as a new column and corresponding 'Vaccination coverage' values\n",
    "df_long = pd.melt(df_pivot, id_vars=['ISO3', 'Year', 'Animal'], var_name='Disease', value_name='Vaccination Coverage')\n",
    "df_new = df_long.dropna(subset=['Vaccination Coverage']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dc11cf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the \"Vaccination coverage\" column from both dataframes for comparison\n",
    "df1_compare = df_new.drop(columns=['Vaccination Coverage']).reset_index(drop=True)\n",
    "df2_compare = df_original.drop(columns=['Vaccination Coverage']).reset_index(drop=True)\n",
    "\n",
    "# Merge to identify rows that are common between dataframe1 and dataframe2, excluding \"Vaccination coverage\"\n",
    "df_diff = pd.merge(df1_compare, df2_compare, how='left', indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d029b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \"Vaccination coverage\" from both dataframes for comparison\n",
    "df1_compare = df_new.drop(columns=['Vaccination Coverage']).reset_index(drop=True)\n",
    "df2_compare = df_original.drop(columns=['Vaccination Coverage'])\n",
    "\n",
    "# Merge  to identify rows that are common between dataframe1 and dataframe2, excluding \"Vaccination coverage\"\n",
    "df_diff = pd.merge(df1_compare, df2_compare, how='left', indicator=True)\n",
    "\n",
    "\n",
    "# Now boolean indexing to keep rows present only in df_new (i.e., 'left_only')\n",
    "df_filtered = df_new.loc[df_diff['_merge'] == 'left_only'].copy()\n",
    "\n",
    "# Add a new column 'Source' with the value 'Imputed'\n",
    "df_filtered['Source'] = 'Imputed'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "92cb99b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "very_final_df=pd.merge(df_filtered, CI_df_vacc_cov, how='left', indicator=False, on=['ISO3','Year','Animal','Disease'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c6ebcc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#USA does not vaccinate against the velogenic strain of newcastle disease (just non-velogenic)\n",
    "very_final_df=very_final_df[~((very_final_df['ISO3']=='USA')&(very_final_df['Disease']=='Newcastle disease (velogenic)'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "50c8d418",
   "metadata": {},
   "outputs": [],
   "source": [
    "very_final_df.to_csv(os.path.join(notebook_dir,'Vaccination Coverage','FINAL_ALL_vaccine_cov_imputations_to_add.csv'),index=False,encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
